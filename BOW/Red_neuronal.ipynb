{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9f078-4287-460a-98bc-ebc796d89f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ca6d8-ab0a-4dda-bb5a-8e88e9cdd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732795e5-2318-4e6c-9a3f-e425d36c0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx_nodules_stable[\"yes stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2fb08-f3d9-49f4-9e74-cb4d785e53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[\"tipo\",\"tecnica\",\"family\",\n",
    "           \"prosthesis\",\"birads\",\"density_mammo\",\"calcifications_benign\",\"ganglio_mamo\",\"density_echo\",\"lymph_benign\",\"lymph_suspicious\",\"simple_cyst\",\"ductal_ectasia\",\n",
    "          \"nodules_echo\",\"nodules_shape\",\"nodules_margin\",\"nodules_echogenicity\",\"nodules_known\",\"nodules_stable\"]\n",
    "\n",
    "TIPO=[\"biopsy report\", \"nodal staging ultrasound report\", \"normal control or revision report\"]\n",
    "TECNICA=[\"only ultrasound study\", \"only mammography study\", \"mammography and ultrasound\"]\n",
    "FAMILY=[\"no family history\", \"first degree\", \"second degree\"]\n",
    "PROSTHESIS=[\"no\",\"yes\"]\n",
    "BIRADS=[\"BI-RADS 0\",\"BI-RADS 1\",\"BI-RADS 2\",\"BI-RADS 3\",\"BI-RADS 4A\",\"BI-RADS 4B\",\"BI-RADS 4C\",\"BI-RADS 5\",\"BI-RADS 6\"]\n",
    "DENSITY_MAMMO=[\"ACR A\",\"ACR B\",\"ACR C\",\"ACR D\",\"unknown\"]\n",
    "CALCIFICATIONS_BENIGN=[\"no\",\"yes\"]\n",
    "GANGLIO_MAMO=[\"no\",\"yes\"]\n",
    "DENSITY_ECHO=[\"homogeneous fibroglandular\",\"heterogeneous fibroglandular\",\"fibroglandular and fat\",\"homogeneous fatty\",\"unknown\"]\n",
    "LYMPH_BENIGN=[\"no\",\"yes\"]\n",
    "LYMPH_SUSPICIOUS=[\"no\",\"yes\"]\n",
    "SIMPLE_CYST=[\"no\",\"yes\"]\n",
    "DUCTAL_ECTASIA=[\"no\",\"yes\"]\n",
    "NODULES_ECHO=[\"no nodules\", \"yes nodules\"]\n",
    "NODULES_SHAPE=[\"oval\",\"round\",\"lobulated\",\"irregular\",\"unknown shape\"]\n",
    "NODULES_MARGIN=[\"circumscribed\",\"spiculated\",\"indistinct\",\"not circumscribed\",\"unknown margin\"]\n",
    "NODULES_ECHOGENICITY=[\"hypoechoic\", \"isoechoic\", \"heterogeneous\",\"complex cystic and solid\",\"unknown echogenicity\"]\n",
    "NODULES_KNOWN=[\"no known\", \"yes known\"]\n",
    "NODULES_STABLE=[\"grown stable\",\"shrunk stable\", \"yes stable\"]\n",
    "\n",
    "\n",
    "TIPO.sort()\n",
    "TECNICA.sort()\n",
    "FAMILY.sort()\n",
    "PROSTHESIS.sort()\n",
    "BIRADS.sort()\n",
    "DENSITY_MAMMO.sort()\n",
    "CALCIFICATIONS_BENIGN.sort()\n",
    "GANGLIO_MAMO.sort()\n",
    "DENSITY_ECHO.sort()\n",
    "LYMPH_BENIGN.sort()\n",
    "SIMPLE_CYST.sort()\n",
    "DUCTAL_ECTASIA.sort()\n",
    "NODULES_ECHO.sort()\n",
    "NODULES_SHAPE.sort()\n",
    "NODULES_MARGIN.sort()\n",
    "NODULES_ECHOGENICITY.sort()\n",
    "NODULES_KNOWN.sort()\n",
    "NODULES_STABLE.sort()\n",
    "\n",
    "\n",
    "word_to_idx_tipo={word:idx for idx,word in enumerate(TIPO)}\n",
    "idx_to_word_tipo={idx:word for idx,word in enumerate(TIPO)}\n",
    "\n",
    "word_to_idx_tecnica={word:idx for idx,word in enumerate(TECNICA)}\n",
    "idx_to_word_tecnica={idx:word for idx,word in enumerate(TECNICA)}\n",
    "\n",
    "word_to_idx_family={word:idx for idx,word in enumerate(FAMILY)}\n",
    "idx_to_word_family={idx:word for idx,word in enumerate(FAMILY)}\n",
    "\n",
    "word_to_idx_prosthesis={word:idx for idx,word in enumerate(PROSTHESIS)}\n",
    "idx_to_word_prosthesis={idx:word for idx,word in enumerate(PROSTHESIS)}\n",
    "\n",
    "word_to_idx_birads={word:idx for idx,word in enumerate(BIRADS)}\n",
    "idx_to_word_birads={idx:word for idx,word in enumerate(BIRADS)}\n",
    "\n",
    "word_to_idx_density_mammo={word:idx for idx,word in enumerate(DENSITY_MAMMO)}\n",
    "idx_to_word_density_mammo={idx:word for idx,word in enumerate(DENSITY_MAMMO)}\n",
    "\n",
    "word_to_idx_calcifications_benign={word:idx for idx,word in enumerate(CALCIFICATIONS_BENIGN)}\n",
    "idx_to_word_calcifications_benign={idx:word for idx,word in enumerate(CALCIFICATIONS_BENIGN)}\n",
    "\n",
    "word_to_idx_ganglio_mamo={word:idx for idx,word in enumerate(GANGLIO_MAMO)}\n",
    "idx_to_word_ganglio_mamo={idx:word for idx,word in enumerate(GANGLIO_MAMO)}\n",
    "\n",
    "word_to_idx_density_echo={word:idx for idx,word in enumerate(DENSITY_ECHO)}\n",
    "idx_to_word_density_echo={idx:word for idx,word in enumerate(DENSITY_ECHO)}\n",
    "\n",
    "word_to_idx_lymph_benign={word:idx for idx,word in enumerate(LYMPH_BENIGN)}\n",
    "idx_to_word_lymph_benign={idx:word for idx,word in enumerate(LYMPH_BENIGN)}\n",
    "\n",
    "word_to_idx_lymph_suspicious={word:idx for idx,word in enumerate(LYMPH_SUSPICIOUS)}\n",
    "idx_to_word_lymph_suspicious={idx:word for idx,word in enumerate(LYMPH_SUSPICIOUS)}\n",
    "\n",
    "word_to_idx_simple_cyst={word:idx for idx,word in enumerate(SIMPLE_CYST)}\n",
    "idx_to_word_simple_cyst={idx:word for idx,word in enumerate(SIMPLE_CYST)}\n",
    "\n",
    "word_to_idx_ductal_ectasia={word:idx for idx,word in enumerate(DUCTAL_ECTASIA)}\n",
    "idx_to_word_ductal_ectasia={idx:word for idx,word in enumerate(DUCTAL_ECTASIA)}\n",
    "\n",
    "word_to_idx_nodules_echo={word:idx for idx,word in enumerate(NODULES_ECHO)}\n",
    "idx_to_word_nodules_echo={idx:word for idx,word in enumerate(NODULES_ECHO)}\n",
    "\n",
    "word_to_idx_nodules_shape={word:idx for idx,word in enumerate(NODULES_SHAPE)}\n",
    "idx_to_word_nodules_shape={idx:word for idx,word in enumerate(NODULES_SHAPE)}\n",
    "\n",
    "word_to_idx_nodules_margin={word:idx for idx,word in enumerate(NODULES_MARGIN)}\n",
    "idx_to_word_nodules_margin={idx:word for idx,word in enumerate(NODULES_MARGIN)}\n",
    "\n",
    "word_to_idx_nodules_echogenicity={word:idx for idx,word in enumerate(NODULES_ECHOGENICITY)}\n",
    "idx_to_word_nodules_echogenicity={idx:word for idx,word in enumerate(NODULES_ECHOGENICITY)}\n",
    "\n",
    "word_to_idx_nodules_known={word:idx for idx,word in enumerate(NODULES_KNOWN)}\n",
    "idx_to_word_nodules_known={idx:word for idx,word in enumerate(NODULES_KNOWN)}\n",
    "\n",
    "word_to_idx_nodules_stable={word:idx for idx,word in enumerate(NODULES_STABLE)}\n",
    "idx_to_word_nodules_stable={idx:word for idx,word in enumerate(NODULES_STABLE)}\n",
    "\n",
    "\n",
    "DICTIONARY={\"tipo\":TIPO,\"tecnica\":TECNICA,\"family\":FAMILY,\"prosthesis\":PROSTHESIS,\"birads\":BIRADS,\"density_mammo\":DENSITY_MAMMO,\"calcifications_benign\":CALCIFICATIONS_BENIGN,\n",
    "            \"ganglio_mamo\":GANGLIO_MAMO,\"density_echo\":DENSITY_ECHO,\"lymph_benign\":LYMPH_BENIGN,\"lymph_suspicious\":LYMPH_SUSPICIOUS,\"simple_cyst\":SIMPLE_CYST,\"ductal_ectasia\":DUCTAL_ECTASIA,\n",
    "           \"nodules_echo\": NODULES_ECHO,\"nodules_shape\":NODULES_SHAPE,\"nodules_margin\":NODULES_MARGIN, \"nodules_echogenicity\":NODULES_ECHOGENICITY, \"nodules_known\":NODULES_KNOWN, \"nodules_stable\":NODULES_STABLE}\n",
    "\n",
    "\n",
    "outputs=[]\n",
    "for tipo in DICTIONARY.values():\n",
    "    outputs+=tipo\n",
    "print(outputs)\n",
    "\n",
    "word_to_idx_out={word:idx for idx,word in enumerate(outputs)}\n",
    "idx_to_word_out={idx:word for idx,word in enumerate(outputs)}\n",
    "import gc\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Descargar recursos de NLTK (si es necesario)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Lista de stop words en español\n",
    "stop_words_es = set(stopwords.words('english'))\n",
    "\n",
    "# Función para eliminar stop words\n",
    "def eliminar_stopwords(texto, stop_words):\n",
    "    palabras = texto.split()\n",
    "    palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]\n",
    "    return \" \".join(palabras_filtradas)\n",
    "\n",
    "\n",
    "def fix_brackets_spaces(texto):\n",
    "    ''' \n",
    "        Introduce espacios por delante y por detrás de los paréntesis.\n",
    "        Esta medida mejora el tokenizado de Spacy\n",
    "    '''\n",
    "    \n",
    "    texto = re.sub(r'([(\\[¿!])', r' \\1', texto)\n",
    "    texto = re.sub(r'([)\\]?¡])', r'\\1 ', texto)\n",
    "                \n",
    "    return texto\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuación\n",
    "    text= fix_brackets_spaces(text)\n",
    "    # print(text)\n",
    "    text=eliminar_stopwords(text, stop_words_es)\n",
    "    # print(text)\n",
    "    return text\n",
    "\n",
    "    \n",
    "def flatten_and_filter_dataset(ground_truth,reports):\n",
    "    \"\"\"\n",
    "    Esta función toma un conjunto de datos en el formato original (con estructura jerárquica)\n",
    "    y devuelve un conjunto de datos plano, donde cada entrada tiene un solo `context`, `question` y `answer`.\n",
    "    \n",
    "    Argumentos:\n",
    "        dataset: Un conjunto de datos en formato original (puede ser train, validation, test).\n",
    "    \n",
    "    Retorno:\n",
    "        Un conjunto de datos de Hugging Face en formato plano, con solo ejemplos completos.\n",
    "    \"\"\"\n",
    "    # Lista para almacenar ejemplos en formato plano\n",
    "    flattened_examples = {}\n",
    "    targets={}\n",
    "    j=0\n",
    "    for i, report in enumerate(reports[\"informes_ingles\"]):\n",
    "        informe=preprocess_text(report)\n",
    "        key=reports[\"keys\"][i]\n",
    "        \n",
    "        if key not in ground_truth.index:\n",
    "            continue\n",
    "        if key in flattened_examples:\n",
    "            continue\n",
    "\n",
    "        n_tipo=np.zeros(len(TIPO))\n",
    "        n_tecnica=np.zeros(len(TECNICA))\n",
    "        n_family=np.zeros(len(FAMILY))\n",
    "        n_prosthesis=np.zeros(len(PROSTHESIS))\n",
    "        n_birads=np.zeros(len(BIRADS))\n",
    "        n_density_mammo=np.zeros(len(DENSITY_MAMMO))\n",
    "        n_calcifications_benign=np.zeros(len(CALCIFICATIONS_BENIGN))\n",
    "        n_ganglio_mamo=np.zeros(len(GANGLIO_MAMO))\n",
    "        n_density_echo=np.zeros(len(DENSITY_ECHO))\n",
    "        n_lymph_benign=np.zeros(len(LYMPH_BENIGN))\n",
    "        n_lymph_suspicious=np.zeros(len(LYMPH_SUSPICIOUS))\n",
    "        n_simple_cyst=np.zeros(len(SIMPLE_CYST))\n",
    "        n_ductal_ectasia=np.zeros(len(DUCTAL_ECTASIA))\n",
    "        n_nodules_echo=np.zeros(len(NODULES_ECHO))\n",
    "        n_nodules_shape=np.zeros(len(NODULES_SHAPE))\n",
    "        n_nodules_margin=np.zeros(len(NODULES_MARGIN))\n",
    "        n_nodules_echogenicity=np.zeros(len(NODULES_ECHOGENICITY))\n",
    "        n_nodules_known=np.zeros(len(NODULES_KNOWN))\n",
    "        n_nodules_stable=np.zeros(len(NODULES_STABLE))\n",
    "        row=ground_truth.loc[key]\n",
    "\n",
    "        #TIPO\n",
    "        normal_control=False\n",
    "        if row[\"Biopsy_report\"]==\"Yes\":\n",
    "            n_tipo[word_to_idx_tipo[\"biopsy report\"]]=1\n",
    "            \n",
    "        elif row[\"Ganglio_report\"]==\"Yes\":\n",
    "            n_tipo[word_to_idx_tipo[\"nodal staging ultrasound report\"]]=1\n",
    "        else:\n",
    "            normal_control=True\n",
    "            n_tipo[word_to_idx_tipo[\"normal control or revision report\"]]=1\n",
    "        \n",
    "        #TECHNIQUE\n",
    "        tecnica=row[\"Technique\"]\n",
    "        # Verificar si el ejemplo tiene preguntas\n",
    "        if tecnica==\"ultrasound\":\n",
    "            n_tecnica[word_to_idx_tecnica[\"only ultrasound study\"]]=1          \n",
    "        elif tecnica==\"mammography\":\n",
    "            n_tecnica[word_to_idx_tecnica[\"only mammography study\"]]=1\n",
    "        elif not pd.isna(tecnica):\n",
    "            n_tecnica[word_to_idx_tecnica[tecnica]]=1\n",
    "        else:\n",
    "            print(key,report)\n",
    "        \n",
    "        # \n",
    "        # HISTORY\n",
    "        #No consideramos las biopsias o las ecografías de estadificación ganglionar.\n",
    "        if normal_control:\n",
    "            \n",
    "            family=row[\"Family_history\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(family,str) or family==\"No\":\n",
    "                n_family[word_to_idx_family[\"no family history\"]]=1         \n",
    "            else:\n",
    "                n_family[word_to_idx_family[family]]=1\n",
    "                \n",
    "            # PROSTHESIS\n",
    "            prosthesis=row[\"Prosthesis\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(prosthesis,str) or prosthesis==\"No\":\n",
    "                n_prosthesis[word_to_idx_prosthesis[\"no\"]]=1        \n",
    "            else:\n",
    "                n_prosthesis[word_to_idx_prosthesis[\"yes\"]]=1\n",
    "    \n",
    "            #BIRADS\n",
    "            birads=row[\"BI-RADS\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(birads,str):\n",
    "                n_birads[word_to_idx_birads[\"unknown\"]]=1           \n",
    "            else:\n",
    "                n_birads[word_to_idx_birads[birads]]=1\n",
    "    \n",
    "            #Density mammo\n",
    "            density_mammo=row[\"Density_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_mammo,str) or density_mammo not in DENSITY_MAMMO:\n",
    "                n_density_mammo[word_to_idx_density_mammo[\"unknown\"]]=1       \n",
    "            else:\n",
    "                n_density_mammo[word_to_idx_density_mammo[density_mammo]]=1\n",
    "\n",
    "            #Lymp nodes mammo\n",
    "            ganglio_mamo=row[\"Ganglio_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ganglio_mamo,str):\n",
    "                n_ganglio_mamo[word_to_idx_ganglio_mamo[\"no\"]]=1            \n",
    "            else:\n",
    "                n_ganglio_mamo[word_to_idx_ganglio_mamo[ganglio_mamo.lower()]]=1\n",
    "\n",
    "            #Calcifications benign\n",
    "            calcifications_benign=row[\"Calcifications_benign_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(calcifications_benign,str):\n",
    "                n_calcifications_benign[word_to_idx_calcifications_benign[\"no\"]]=1       \n",
    "            else:\n",
    "                n_calcifications_benign[word_to_idx_calcifications_benign[calcifications_benign.lower()]]=1\n",
    "\n",
    "        \n",
    "    \n",
    "            #Density echo\n",
    "            density_echo=row[\"Density_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_echo,str)or density_echo not in DENSITY_ECHO:\n",
    "                n_density_echo[word_to_idx_density_echo[\"unknown\"]]=1         \n",
    "            else:\n",
    "                n_density_echo[word_to_idx_density_echo[density_echo]]=1\n",
    "\n",
    "            #Benign lymph nodes\n",
    "            simple_cyst=row[\"simple_cyst_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(simple_cyst,str):\n",
    "                n_simple_cyst[word_to_idx_simple_cyst[\"no\"]]=1         \n",
    "            else:\n",
    "                n_simple_cyst[word_to_idx_simple_cyst[simple_cyst.lower()]]=1\n",
    "            #Suspicious lymph nodes\n",
    "            lymph_suspicious=row[\"Ganglio_suspicious_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(lymph_suspicious,str):\n",
    "                n_lymph_suspicious[word_to_idx_lymph_suspicious[\"no\"]]=1         \n",
    "            else:\n",
    "                n_lymph_suspicious[word_to_idx_lymph_suspicious[lymph_suspicious.lower()]]=1\n",
    "\n",
    "            #Benign lymph nodes\n",
    "            lymph_benign=row[\"Ganglio_benign_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            \n",
    "            if not isinstance(lymph_benign,str):\n",
    "                n_lymph_benign[word_to_idx_lymph_benign[\"no\"]]=1           \n",
    "            else:\n",
    "                n_lymph_benign[word_to_idx_lymph_benign[lymph_benign.lower()]]=1\n",
    "            \n",
    "            #Ductal ectasia\n",
    "            ductal_ectasia=row[\"Ductal_ectasia_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ductal_ectasia,str):\n",
    "                n_ductal_ectasia[word_to_idx_ductal_ectasia[\"no\"]]=1    \n",
    "            else:\n",
    "                n_ductal_ectasia[word_to_idx_ductal_ectasia[ductal_ectasia.lower()]]=1\n",
    "\n",
    "            nodules_echo=row[\"Nodules_eco\"]\n",
    "            nodules_bool=False\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(nodules_echo,str) and not isinstance(nodules_echo,int):\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"no nodules\"]]=1\n",
    "            elif isinstance(nodules_echo,str) and nodules_echo==\"No\":\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"no nodules\"]]=1\n",
    "            else:\n",
    "                nodules_bool=True\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"yes nodules\"]]=1\n",
    "            if nodules_bool:\n",
    "                #Density echo\n",
    "                nodules_shape=row[\"Shape_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_shape,str)or nodules_shape not in NODULES_SHAPE:\n",
    "                    n_nodules_shape[word_to_idx_nodules_shape[\"unknown shape\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_shape[word_to_idx_nodules_shape[nodules_shape]]=1\n",
    "               \n",
    "\n",
    "                nodules_margin=row[\"Margin_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_margin,str)or nodules_margin not in NODULES_MARGIN:\n",
    "                    n_nodules_margin[word_to_idx_nodules_margin[\"unknown margin\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_margin[word_to_idx_nodules_margin[nodules_margin]]=1\n",
    "                \n",
    "\n",
    "                nodules_echogenicity=row[\"Echogenicity_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_echogenicity,str)or nodules_echogenicity not in NODULES_ECHOGENICITY:\n",
    "                    n_nodules_echogenicity[word_to_idx_nodules_echogenicity[\"unknown echogenicity\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_echogenicity[word_to_idx_nodules_echogenicity[nodules_echogenicity]]=1\n",
    "                \n",
    "\n",
    "                #Nodules echo known\n",
    "                nodules_known=row[\"new_eco_1\"]\n",
    "                known_bool=False\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_known,str):\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"unknown known\"]]=1\n",
    "                elif nodules_known==\"No\":\n",
    "                    known_bool=True\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"yes known\"]]=1    \n",
    "                else:\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"no known\"]]=1\n",
    "                \n",
    "                if known_bool:\n",
    "                    #Nodules echo stable\n",
    "                    nodules_stable=row[\"Stable_eco_1\"]\n",
    "                    # Verificar si el ejemplo tiene preguntas\n",
    "                    if not isinstance(nodules_stable,str):\n",
    "                        n_nodules_stable[word_to_idx_nodules_stable[\"unknown stable\"]]=1\n",
    "                    else:\n",
    "                        n_nodules_stable[word_to_idx_nodules_stable[nodules_stable.lower()+\" stable\"]]=1\n",
    "                    \n",
    "        \n",
    "        targets[key]=[n_tipo,n_tecnica,n_family,\n",
    "        n_prosthesis,n_birads,n_density_mammo,n_calcifications_benign,n_ganglio_mamo,n_density_echo,\n",
    "        n_lymph_benign,n_lymph_suspicious,n_simple_cyst,n_ductal_ectasia,n_nodules_echo,n_nodules_shape,\n",
    "        n_nodules_margin,n_nodules_echogenicity,n_nodules_known,n_nodules_stable] \n",
    "        \n",
    "        flattened_examples[key]=informe\n",
    "    return flattened_examples,targets\n",
    "inputs,targets = flatten_and_filter_dataset(ground_truth,report_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8105a4-8589-4880-bc77-c011a9bce154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar CountVectorizer al dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=2)\n",
    "# vectorizer =CountVectorizer(max_features=2000, min_df=2)\n",
    "X = vectorizer.fit_transform(inputs.values()).toarray()\n",
    "vocabulario = vectorizer.get_feature_names_out()\n",
    "num_unique_words = len(vocabulario)\n",
    "print(f\"Número de palabras únicas en el dataset: {num_unique_words}\")\n",
    "\n",
    "dataset_final={}\n",
    "for ind,key in enumerate(inputs.keys()):\n",
    "    dataset_final[key]=X[ind]\n",
    "dataset_final=pd.DataFrame.from_dict(dataset_final,orient='index')\n",
    "targets=pd.DataFrame.from_dict(targets,orient='index')  \n",
    "targets.columns=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd0821-c52f-425f-9980-3844752bd2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import random\n",
    "import math\n",
    "\n",
    "inp=k.Input(shape=(len(dataset_final.iloc[0]),))\n",
    "dense=Dense(64,activation=\"gelu\")(inp)\n",
    "#         dense=Dense(12,activation=\"relu\")(dense)\n",
    "drop = Dropout(0.3)(dense)\n",
    "outputs={}\n",
    "for question in questions:\n",
    "    dic=DICTIONARY[question]\n",
    "    outputs[question]=Dense(len(dic),activation=\"softmax\")(drop)\n",
    "epochs=45\n",
    "lr_schedule = k.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=250,\n",
    "    decay_rate=0.98)\n",
    "MLP=k.Model(inputs=inp,outputs=outputs)\n",
    "MLP.compile(loss=\"categorical_crossentropy\",optimizer=k.optimizers.Adam(learning_rate=lr_schedule),metrics=[\"accuracy\"])\n",
    "model=MLP\n",
    "# Diferenciamos el fit cuando el resultado es categorical o no.\n",
    "# history=MLP.fit(dataset_final.array(),targets.array(),epochs=epochs,batch_size=20,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717871d1-05de-4e86-b729-1340792fa12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow.keras.backend as K\n",
    "def masked_categorical_crossentropy():\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Máscara booleana: 1 si la muestra es válida (tiene una clase activa), 0 si es todo ceros\n",
    "        mask = tf.reduce_sum(y_true, axis=-1)  # suma por clase → 0 si todo el vector es [0, 0, 0]\n",
    "        valid = tf.cast(mask > 0, tf.float32)  # 1 si es válida\n",
    "\n",
    "        # Cálculo de la entropía cruzada normal\n",
    "        loss = K.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Aplicar la máscara → pérdida 0 para entradas inválidas\n",
    "        loss = loss * valid\n",
    "\n",
    "        # Promediar solo sobre muestras válidas\n",
    "        num_valid = tf.reduce_sum(valid)\n",
    "        return tf.math.divide_no_nan(tf.reduce_sum(loss), num_valid)\n",
    "    \n",
    "    return loss_fn\n",
    "\n",
    "def test_train_split(i, X, Y):\n",
    "    random.seed(1)\n",
    "    \n",
    "    def obtener_grupo(nombre):\n",
    "        if '_copy' in nombre:\n",
    "            return nombre.split('_copy')[0]\n",
    "        return nombre\n",
    "    \n",
    "    agrupaciones = pd.DataFrame(index=X.index)\n",
    "    agrupaciones['grupo'] = agrupaciones.index.map(obtener_grupo)\n",
    "    grupos_unicos = agrupaciones['grupo'].unique()\n",
    "    \n",
    "    cv_tamaño = math.ceil(len(grupos_unicos) / 10)\n",
    "    test_grupos = grupos_unicos[i * cv_tamaño : min(len(grupos_unicos), (i + 1) * cv_tamaño)]\n",
    "    \n",
    "    test_indices = agrupaciones[agrupaciones['grupo'].isin(test_grupos)].index\n",
    "    train_indices = agrupaciones[~agrupaciones['grupo'].isin(test_grupos)].index\n",
    "    \n",
    "    test = X.loc[test_indices].sort_index()\n",
    "    test_y = Y.loc[test_indices].sort_index()\n",
    "    \n",
    "    train = X.loc[train_indices].sort_index().sample(frac=1, random_state=1)\n",
    "    train_y = Y.loc[train.index]\n",
    "    \n",
    "    return train, train_y, test, test_y, list(test.index)\n",
    "\n",
    "def cross_validation(X, Y):\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "    \n",
    "    predicted = {question:[] for question in questions}\n",
    "    tested = {question:[] for question in questions}\n",
    "    acc_cv = []\n",
    "    ind_cv = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        train, train_y, test, test_y, ind = test_train_split(i, X, Y)\n",
    "        \n",
    "        inp = Input(shape=(X.shape[1],))\n",
    "        dense = Dense(64, activation=\"gelu\")(inp)\n",
    "        drop = Dropout(0.1)(dense)\n",
    "        # dense = Dense(32, activation=\"gelu\")(dense)\n",
    "        outputs = {}\n",
    "        for question in questions:\n",
    "            dic = DICTIONARY[question]\n",
    "            outputs[question] = Dense(len(dic), activation=\"softmax\", name=question)(drop)\n",
    "        \n",
    "        lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=0.005,\n",
    "            decay_steps=60,\n",
    "            decay_rate=0.98\n",
    "        )\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=outputs)\n",
    "        model.compile(\n",
    "            loss={question: masked_categorical_crossentropy() for question in questions},\n",
    "            optimizer=Adam(learning_rate=lr_schedule),\n",
    "            metrics={question: \"accuracy\" for question in questions}\n",
    "        )\n",
    "        \n",
    "        train_y_dict = {question: np.vstack(train_y[question]).astype(\"float32\") for question in questions}\n",
    "\n",
    "        # Verificar los shapes y tipos de datos antes de entrenar\n",
    "        print(\"Shapes y tipos de datos antes de model.fit():\")\n",
    "        print(f\"train shape: {train.shape}, dtype: {train.dtypes}\")\n",
    "        for key, value in train_y_dict.items():\n",
    "            print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        \n",
    "        # Entrenamiento\n",
    "        model.fit(np.array(train, dtype=\"float32\"), train_y_dict, epochs=150, verbose=1,batch_size=16)\n",
    "        \n",
    "        pred_dict = model.predict(test)\n",
    "        test_y = {question: np.vstack(test_y[question]).astype(\"float32\") for question in questions}\n",
    "        \n",
    "        for question in questions:\n",
    "            \n",
    "            pred_class = np.argmax(pred_dict[question], axis=1)\n",
    "            test_class = np.argmax(test_y[question], axis=1)\n",
    "            valid_mask = np.sum(test_y[question], axis=1) == 1\n",
    "    \n",
    "            # Apply the mask\n",
    "            pred_class = pred_class[valid_mask]\n",
    "            test_class = test_class[valid_mask]\n",
    "            \n",
    "            predicted[question]+=list(pred_class)\n",
    "            tested[question]+=list(test_class)\n",
    "            acc = accuracy_score(test_class, pred_class)\n",
    "            acc_cv.append(acc)\n",
    "            \n",
    "            print(f\"Accuracy ({question}): {acc:.2f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(test_class, pred_class))\n",
    "        \n",
    "        ind_cv.append(ind)\n",
    "    print(\"FINAL\")\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "        print(accuracy_score(tested[question], predicted[question]))\n",
    "        print(classification_report(tested[question],predicted[question]))\n",
    "    acc_final = np.mean(acc_cv)\n",
    "    acc_std = np.std(acc_cv)\n",
    "    return predicted,tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bc0a7-840c-4c4e-b88d-4aacb0146723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted, tested=cross_validation(dataset_final,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2217f61-433b-48d1-8351-2652a5a2d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[]\n",
    "f1s=[]\n",
    "from sklearn.metrics import f1_score\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    print(accuracy_score(tested[question], predicted[question]))\n",
    "    acc.append(accuracy_score(tested[question], predicted[question]))\n",
    "    # print(classification_report(tested[question],predicted[question]))\n",
    "    f1=f1_score(tested[question],predicted[question],average='macro')\n",
    "    print(f1)\n",
    "    f1s.append(f1)\n",
    "print(f1s)\n",
    "print(np.mean(acc), np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a70ee-d8bf-44a1-a934-50f7daa9b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        print(question)\n",
    "        print(accuracy_score(tested[question], predicted[question]))\n",
    "        print(classification_report(tested[question],predicted[question]))\n",
    "        f1=f1_score(tested[question],predicted[question],average=average)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a820c59-e19f-4f67-9fcc-6f4b4cc5e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51919b02-793f-413d-9dbf-ee8da17b76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff27d-11e4-4c59-839a-875a2ac53ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def masked_categorical_crossentropy():\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Máscara booleana: 1 si la muestra es válida (tiene una clase activa), 0 si es todo ceros\n",
    "        mask = tf.reduce_sum(y_true, axis=-1)  # suma por clase → 0 si todo el vector es [0, 0, 0]\n",
    "        valid = tf.cast(mask > 0, tf.float32)  # 1 si es válida\n",
    "\n",
    "        # Cálculo de la entropía cruzada normal\n",
    "        loss = K.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Aplicar la máscara → pérdida 0 para entradas inválidas\n",
    "        loss = loss * valid\n",
    "\n",
    "        # Promediar solo sobre muestras válidas\n",
    "        num_valid = tf.reduce_sum(valid)\n",
    "        return tf.math.divide_no_nan(tf.reduce_sum(loss), num_valid)\n",
    "    \n",
    "    return loss_fn\n",
    "def cross_validation(X, Y,test,test_y):\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "    \n",
    "    predicted = {question:[] for question in questions}\n",
    "    tested = {question:[] for question in questions}\n",
    "    acc_cv = []\n",
    "    ind_cv = []\n",
    "    \n",
    "    \n",
    "    train = X.sort_index().sample(frac=1, random_state=1)\n",
    "    train_y = Y.loc[train.index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    inp = Input(shape=(X.shape[1],))\n",
    "    dense = Dense(64, activation=\"gelu\")(inp)\n",
    "    drop = Dropout(0.1)(dense)\n",
    "    # dense = Dense(32, activation=\"gelu\")(dense)\n",
    "    outputs = {}\n",
    "    for question in questions:\n",
    "        dic = DICTIONARY[question]\n",
    "        outputs[question] = Dense(len(dic), activation=\"softmax\", name=question)(drop)\n",
    "    \n",
    "    lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=0.005,\n",
    "            decay_steps=60,\n",
    "            decay_rate=0.98\n",
    "        )\n",
    "        \n",
    "    model = Model(inputs=inp, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss={question: masked_categorical_crossentropy() for question in questions},\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        metrics={question: \"accuracy\" for question in questions}\n",
    "    )\n",
    "    \n",
    "    train_y_dict = {question: np.vstack(train_y[question]).astype(\"float32\") for question in questions}\n",
    "\n",
    "    # Verificar los shapes y tipos de datos antes de entrenar\n",
    "    print(\"Shapes y tipos de datos antes de model.fit():\")\n",
    "    print(f\"train shape: {train.shape}, dtype: {train.dtypes}\")\n",
    "    for key, value in train_y_dict.items():\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(np.array(train, dtype=\"float32\"), train_y_dict, epochs=150, verbose=1,batch_size=16)\n",
    "    \n",
    "    pred_dict = model.predict(test)\n",
    "    test_y = {question: np.vstack(test_y[question]).astype(\"float32\") for question in questions}\n",
    "    \n",
    "    for question in questions:\n",
    "        pred_class = np.argmax(pred_dict[question], axis=1)\n",
    "        test_class = np.argmax(test_y[question], axis=1)\n",
    "        valid_mask = np.sum(test_y[question], axis=1) == 1\n",
    "    \n",
    "        # Apply the mask\n",
    "        pred_class = pred_class[valid_mask]\n",
    "        test_class = test_class[valid_mask]\n",
    "        print(pred_class)\n",
    "        predicted[question]+=list(pred_class)\n",
    "        tested[question]+=list(test_class)\n",
    "        acc = accuracy_score(test_class, pred_class)\n",
    "        acc_cv.append(acc)\n",
    "        \n",
    "        print(f\"Accuracy ({question}): {acc:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(test_class, pred_class))\n",
    "    \n",
    "    print(\"FINAL\")\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "        print(accuracy_score(tested[question], predicted[question]))\n",
    "        print(classification_report(tested[question],predicted[question]))\n",
    "\n",
    "    return predicted,tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae496e0-2684-471c-b0ea-ffb491185370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_test=pd.read_excel(\"../data/data_test.xlsx\",index_col=\"Report\")\n",
    "ground_truth_test=ground_truth_test[ground_truth_test[\"Eliminar\"]!=\"Yes\"]\n",
    "\n",
    "with open(\"../data/report_data_test_ingles_v2.pkl\", 'rb') as file:  # 'rb' mode is for reading binary files\n",
    "    report_data_test = pickle.load(file)\n",
    "def flatten_and_filter_dataset(ground_truth,reports):\n",
    "    \"\"\"\n",
    "    Esta función toma un conjunto de datos en el formato original (con estructura jerárquica)\n",
    "    y devuelve un conjunto de datos plano, donde cada entrada tiene un solo `context`, `question` y `answer`.\n",
    "    \n",
    "    Argumentos:\n",
    "        dataset: Un conjunto de datos en formato original (puede ser train, validation, test).\n",
    "    \n",
    "    Retorno:\n",
    "        Un conjunto de datos de Hugging Face en formato plano, con solo ejemplos completos.\n",
    "    \"\"\"\n",
    "    # Lista para almacenar ejemplos en formato plano\n",
    "    flattened_examples = {}\n",
    "    targets={}\n",
    "    j=0\n",
    "    for i, report in enumerate(reports[\"informes_ingles\"]):\n",
    "        informe=preprocess_text(report)\n",
    "        key=reports[\"keys\"][i]\n",
    "        \n",
    "        if key not in ground_truth_test.index:\n",
    "            continue\n",
    "        \n",
    "        if key in flattened_examples:\n",
    "            continue\n",
    "\n",
    "        n_tipo=np.zeros(len(TIPO))\n",
    "        n_tecnica=np.zeros(len(TECNICA))\n",
    "        n_family=np.zeros(len(FAMILY))\n",
    "        n_prosthesis=np.zeros(len(PROSTHESIS))\n",
    "        n_birads=np.zeros(len(BIRADS))\n",
    "        n_density_mammo=np.zeros(len(DENSITY_MAMMO))\n",
    "        n_calcifications_benign=np.zeros(len(CALCIFICATIONS_BENIGN))\n",
    "        n_ganglio_mamo=np.zeros(len(GANGLIO_MAMO))\n",
    "        n_density_echo=np.zeros(len(DENSITY_ECHO))\n",
    "        n_lymph_benign=np.zeros(len(LYMPH_BENIGN))\n",
    "        n_lymph_suspicious=np.zeros(len(LYMPH_SUSPICIOUS))\n",
    "        n_simple_cyst=np.zeros(len(SIMPLE_CYST))\n",
    "        n_ductal_ectasia=np.zeros(len(DUCTAL_ECTASIA))\n",
    "        n_nodules_echo=np.zeros(len(NODULES_ECHO))\n",
    "        n_nodules_shape=np.zeros(len(NODULES_SHAPE))\n",
    "        n_nodules_margin=np.zeros(len(NODULES_MARGIN))\n",
    "        n_nodules_echogenicity=np.zeros(len(NODULES_ECHOGENICITY))\n",
    "        n_nodules_known=np.zeros(len(NODULES_KNOWN))\n",
    "        n_nodules_stable=np.zeros(len(NODULES_STABLE))\n",
    "        row=ground_truth.loc[key]\n",
    "\n",
    "        #TIPO\n",
    "        normal_control=False\n",
    "        if row[\"Biopsy_report\"].lower()==\"yes\":\n",
    "            n_tipo[word_to_idx_tipo[\"biopsy report\"]]=1\n",
    "            \n",
    "        elif row[\"Ganglio_report\"].lower()==\"yes\":\n",
    "            n_tipo[word_to_idx_tipo[\"nodal staging ultrasound report\"]]=1\n",
    "        else:\n",
    "            normal_control=True\n",
    "            n_tipo[word_to_idx_tipo[\"normal control or revision report\"]]=1\n",
    "        \n",
    "        #TECHNIQUE\n",
    "        tecnica=row[\"Technique\"]\n",
    "        # Verificar si el ejemplo tiene preguntas\n",
    "        if tecnica==\"only ultrasound study\":\n",
    "            n_tecnica[word_to_idx_tecnica[\"only ultrasound study\"]]=1          \n",
    "        elif tecnica==\"only mammography study\":\n",
    "            n_tecnica[word_to_idx_tecnica[\"only mammography study\"]]=1\n",
    "        elif not pd.isna(tecnica):\n",
    "            n_tecnica[word_to_idx_tecnica[tecnica]]=1\n",
    "        else:\n",
    "            print(key,report)\n",
    "        \n",
    "        # \n",
    "        # HISTORY\n",
    "        #No consideramos las biopsias o las ecografías de estadificación ganglionar.\n",
    "        if normal_control:\n",
    "            \n",
    "            family=row[\"Family_history\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(family,str) or family.lower()==\"no\":\n",
    "                n_family[word_to_idx_family[\"no family history\"]]=1   \n",
    "            elif family not in word_to_idx_out:\n",
    "                n_family[word_to_idx_family[\"no family history\"]]=1\n",
    "            else:\n",
    "                n_family[word_to_idx_family[family]]=1\n",
    "                \n",
    "            # PROSTHESIS\n",
    "            prosthesis=row[\"Prosthesis\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(prosthesis,str) or prosthesis.lower()==\"no\":\n",
    "                n_prosthesis[word_to_idx_prosthesis[\"no\"]]=1   \n",
    "            \n",
    "            else:\n",
    "                n_prosthesis[word_to_idx_prosthesis[\"yes\"]]=1\n",
    "    \n",
    "            #BIRADS\n",
    "            birads=row[\"BI-RADS\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(birads,str):\n",
    "                n_birads[word_to_idx_birads[\"unknown\"]]=1           \n",
    "            else:\n",
    "                n_birads[word_to_idx_birads[birads]]=1\n",
    "    \n",
    "            #Density mammo\n",
    "            density_mammo=row[\"Density_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_mammo,str) or density_mammo not in DENSITY_MAMMO:\n",
    "                n_density_mammo[word_to_idx_density_mammo[\"unknown\"]]=1       \n",
    "            else:\n",
    "                n_density_mammo[word_to_idx_density_mammo[density_mammo]]=1\n",
    "\n",
    "            #Lymp nodes mammo\n",
    "            ganglio_mamo=row[\"Ganglio_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ganglio_mamo,str):\n",
    "                n_ganglio_mamo[word_to_idx_ganglio_mamo[\"no\"]]=1            \n",
    "            else:\n",
    "                n_ganglio_mamo[word_to_idx_ganglio_mamo[ganglio_mamo.lower()]]=1\n",
    "\n",
    "            #Calcifications benign\n",
    "            calcifications_benign=row[\"Calcifications_benign_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(calcifications_benign,str):\n",
    "                n_calcifications_benign[word_to_idx_calcifications_benign[\"no\"]]=1       \n",
    "            else:\n",
    "                n_calcifications_benign[word_to_idx_calcifications_benign[calcifications_benign.lower()]]=1\n",
    "\n",
    "        \n",
    "    \n",
    "            #Density echo\n",
    "            density_echo=row[\"Density_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_echo,str) or density_echo not in DENSITY_ECHO:\n",
    "                n_density_echo[word_to_idx_density_echo[\"unknown\"]]=1         \n",
    "            else:\n",
    "                n_density_echo[word_to_idx_density_echo[density_echo]]=1\n",
    "\n",
    "            #Benign lymph nodes\n",
    "            simple_cyst=row[\"simple_cyst_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(simple_cyst,str):\n",
    "                n_simple_cyst[word_to_idx_simple_cyst[\"no\"]]=1         \n",
    "            else:\n",
    "                n_simple_cyst[word_to_idx_simple_cyst[simple_cyst.lower()]]=1\n",
    "            #Suspicious lymph nodes\n",
    "            lymph_suspicious=row[\"Ganglio_suspicious_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(lymph_suspicious,str):\n",
    "                n_lymph_suspicious[word_to_idx_lymph_suspicious[\"no\"]]=1         \n",
    "            else:\n",
    "                n_lymph_suspicious[word_to_idx_lymph_suspicious[lymph_suspicious.lower()]]=1\n",
    "\n",
    "            #Benign lymph nodes\n",
    "            lymph_benign=row[\"Ganglio_benign_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            \n",
    "            if not isinstance(lymph_benign,str):\n",
    "                n_lymph_benign[word_to_idx_lymph_benign[\"no\"]]=1           \n",
    "            else:\n",
    "                n_lymph_benign[word_to_idx_lymph_benign[lymph_benign.lower()]]=1\n",
    "            \n",
    "            #Ductal ectasia\n",
    "            ductal_ectasia=row[\"Ductal_ectasia_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ductal_ectasia,str):\n",
    "                n_ductal_ectasia[word_to_idx_ductal_ectasia[\"no\"]]=1    \n",
    "            else:\n",
    "                n_ductal_ectasia[word_to_idx_ductal_ectasia[ductal_ectasia.lower()]]=1\n",
    "\n",
    "            nodules_echo=row[\"Nodules_eco\"]\n",
    "            nodules_bool=False\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(nodules_echo,str) and not isinstance(nodules_echo,int):\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"no nodules\"]]=1\n",
    "            elif isinstance(nodules_echo,str) and nodules_echo.lower()==\"no\":\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"no nodules\"]]=1\n",
    "            else:\n",
    "                nodules_bool=True\n",
    "                n_nodules_echo[word_to_idx_nodules_echo[\"yes nodules\"]]=1\n",
    "            if nodules_bool:\n",
    "                #Density echo\n",
    "                nodules_shape=row[\"Shape_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_shape,str)or nodules_shape not in NODULES_SHAPE:\n",
    "                    n_nodules_shape[word_to_idx_nodules_shape[\"unknown shape\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_shape[word_to_idx_nodules_shape[nodules_shape]]=1\n",
    "               \n",
    "\n",
    "                nodules_margin=row[\"Margin_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_margin,str)or nodules_margin not in NODULES_MARGIN:\n",
    "                    n_nodules_margin[word_to_idx_nodules_margin[\"unknown margin\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_margin[word_to_idx_nodules_margin[nodules_margin]]=1\n",
    "                \n",
    "\n",
    "                nodules_echogenicity=row[\"Echogenicity_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_echogenicity,str)or nodules_echogenicity not in NODULES_ECHOGENICITY:\n",
    "                    n_nodules_echogenicity[word_to_idx_nodules_echogenicity[\"unknown echogenicity\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_echogenicity[word_to_idx_nodules_echogenicity[nodules_echogenicity]]=1\n",
    "                \n",
    "\n",
    "                #Nodules echo known\n",
    "                nodules_known=row[\"new_eco_1\"]\n",
    "                known_bool=False\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_known,str):\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"unknown known\"]]=1\n",
    "                elif nodules_known.lower()==\"no\":\n",
    "                    known_bool=True\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"yes known\"]]=1    \n",
    "                else:\n",
    "                    n_nodules_known[word_to_idx_nodules_known[\"no known\"]]=1\n",
    "                \n",
    "                if known_bool:\n",
    "                    #Nodules echo stable\n",
    "                    nodules_stable=row[\"Stable_eco_1\"]\n",
    "                    # Verificar si el ejemplo tiene preguntas\n",
    "                    if not isinstance(nodules_stable,str):\n",
    "                        n_nodules_stable[word_to_idx_nodules_stable[\"unknown stable\"]]=1\n",
    "                    else:\n",
    "                        n_nodules_stable[word_to_idx_nodules_stable[nodules_stable.lower()+\" stable\"]]=1\n",
    "                    \n",
    "        \n",
    "        targets[key]=[n_tipo,n_tecnica,n_family,\n",
    "        n_prosthesis,n_birads,n_density_mammo,n_calcifications_benign,n_ganglio_mamo,n_density_echo,\n",
    "        n_lymph_benign,n_lymph_suspicious,n_simple_cyst,n_ductal_ectasia,n_nodules_echo,n_nodules_shape,\n",
    "        n_nodules_margin,n_nodules_echogenicity,n_nodules_known,n_nodules_stable] \n",
    "        \n",
    "        flattened_examples[key]=informe\n",
    "    return flattened_examples,targets\n",
    "inputs_test,targets_test = flatten_and_filter_dataset(ground_truth_test,report_data_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e0a6a-fc94-4809-af12-a09128fb891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "\n",
    "X = vectorizer.transform(inputs_test.values()).toarray()\n",
    "vocabulario = vectorizer.get_feature_names_out()\n",
    "num_unique_words = len(vocabulario)\n",
    "print(f\"Número de palabras únicas en el dataset: {num_unique_words}\")\n",
    "\n",
    "dataset_final_test={}\n",
    "for ind,key in enumerate(inputs_test.keys()):\n",
    "    dataset_final_test[key]=X[ind]\n",
    "dataset_final_test=pd.DataFrame.from_dict(dataset_final_test,orient='index')\n",
    "targets_test=pd.DataFrame.from_dict(targets_test,orient='index')  \n",
    "targets_test.columns=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232485d-3270-48aa-8625-664e1bd3e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a03f15-afde-4a26-80d6-3115e7abe7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,tested=cross_validation(dataset_final,targets,dataset_final_test,targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e466225-ad92-4b3a-b414-0b02de6549d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx_family\n",
    "idx_to_word_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1f997-1072-4663-87cf-91fbf1920a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[]\n",
    "f1s=[]\n",
    "from sklearn.metrics import f1_score\n",
    "for question in questions:\n",
    "    print(len(tested[question]))\n",
    "    if question==\"family\":\n",
    "        print(tested[question])\n",
    "        tested[question]=[5]+tested[question][1:]\n",
    "        print(predicted[question])\n",
    "        print(tested[question])\n",
    "    print(question)\n",
    "    print(accuracy_score(tested[question], predicted[question]))\n",
    "    acc.append(accuracy_score(tested[question], predicted[question]))\n",
    "    # print(classification_report(tested[question],predicted[question]))\n",
    "    f1=f1_score(tested[question],predicted[question],average='macro')\n",
    "    print(f1)\n",
    "    f1s.append(f1)\n",
    "print(f1s)\n",
    "print(np.mean(acc), np.mean(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12 MachineLearning GPU",
   "language": "python",
   "name": "ml312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
