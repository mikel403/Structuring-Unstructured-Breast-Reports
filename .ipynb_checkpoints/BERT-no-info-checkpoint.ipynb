{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4a11f-1c1c-41f6-a948-de4ca5536591",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[\"tipo\",\"tecnica\",\"family\",\n",
    "           \"prosthesis\",\"birads\",\"density_mammo\",\"calcifications_benign\",\"ganglio_mamo\",\"density_echo\",\"lymph_benign\",\"lymph_suspicious\",\"simple_cyst\",\"ductal_ectasia\"]\n",
    "\n",
    "TIPO=[\"biopsy report\", \"nodal staging ultrasound report\", \"normal control or revision report\"]\n",
    "TECNICA=[\"only ultrasound study\", \"only mammography study\", \"mammography and ultrasound\"]\n",
    "FAMILY=[\"no family history\", \"first degree\", \"second degree\"]\n",
    "PROSTHESIS=[\"no prosthesis\",\"yes prosthesis\"]\n",
    "BIRADS=[\"BI-RADS 0\",\"BI-RADS 1\",\"BI-RADS 2\",\"BI-RADS 3\",\"BI-RADS 4A\",\"BI-RADS 4B\",\"BI-RADS 4C\",\"BI-RADS 5\",\"BI-RADS 6\"]\n",
    "DENSITY_MAMMO=[\"ACR A\",\"ACR B\",\"ACR C\",\"ACR D\",\"unknown density mammo\"]\n",
    "CALCIFICATIONS_BENIGN=[\"no calcifications\",\"yes calcifications\"]\n",
    "GANGLIO_MAMO=[\"no ganglio\",\"yes ganglio\"]\n",
    "DENSITY_ECHO=[\"homogeneous fibroglandular\",\"heterogeneous fibroglandular\",\"fibroglandular and fat\",\"homogeneous fatty\",\"unknown density echo\"]\n",
    "LYMPH_BENIGN=[\"no lymph benign\",\"yes lymph benign\"]\n",
    "LYMPH_SUSPICIOUS=[\"no lymph suspicious\",\"yes lymph suspicious\"]\n",
    "SIMPLE_CYST=[\"no cyst\",\"yes cyst\"]\n",
    "DUCTAL_ECTASIA=[\"no ectasia\",\"yes ectasia\"]\n",
    "NODULES_ECHO=[\"no nodules\", \"yes nodules\"]\n",
    "NODULES_SHAPE=[\"oval\",\"round\",\"lobulated\",\"irregular\",\"unknown shape\"]\n",
    "NODULES_MARGIN=[\"circumscribed\",\"spiculated\",\"indistinct\",\"not circumscribed\",\"unknown margin\"]\n",
    "NODULES_ECHOGENICITY=[\"hypoechoic\", \"isoechoic\", \"heterogeneous\",\"complex cystic and solid\",\"unknown echogenicity\"]\n",
    "NODULES_KNOWN=[\"no known\", \"yes known\"]\n",
    "NODULES_STABLE=[\"grown stable\",\"shrunk stable\", \"yes stable\"]\n",
    "\n",
    "\n",
    "TIPO.sort()\n",
    "TECNICA.sort()\n",
    "FAMILY.sort()\n",
    "PROSTHESIS.sort()\n",
    "BIRADS.sort()\n",
    "DENSITY_MAMMO.sort()\n",
    "CALCIFICATIONS_BENIGN.sort()\n",
    "GANGLIO_MAMO.sort()\n",
    "DENSITY_ECHO.sort()\n",
    "LYMPH_BENIGN.sort()\n",
    "SIMPLE_CYST.sort()\n",
    "DUCTAL_ECTASIA.sort()\n",
    "NODULES_ECHO.sort()\n",
    "NODULES_SHAPE.sort()\n",
    "NODULES_MARGIN.sort()\n",
    "NODULES_ECHOGENICITY.sort()\n",
    "NODULES_KNOWN.sort()\n",
    "NODULES_STABLE.sort()\n",
    "\n",
    "\n",
    "word_to_idx_tipo={word:idx for idx,word in enumerate(TIPO)}\n",
    "idx_to_word_tipo={idx:word for idx,word in enumerate(TIPO)}\n",
    "\n",
    "word_to_idx_tecnica={word:idx for idx,word in enumerate(TECNICA)}\n",
    "idx_to_word_tecnica={idx:word for idx,word in enumerate(TECNICA)}\n",
    "\n",
    "word_to_idx_family={word:idx for idx,word in enumerate(FAMILY)}\n",
    "idx_to_word_family={idx:word for idx,word in enumerate(FAMILY)}\n",
    "\n",
    "word_to_idx_prosthesis={word:idx for idx,word in enumerate(PROSTHESIS)}\n",
    "idx_to_word_prosthesis={idx:word for idx,word in enumerate(PROSTHESIS)}\n",
    "\n",
    "word_to_idx_birads={word:idx for idx,word in enumerate(BIRADS)}\n",
    "idx_to_word_birads={idx:word for idx,word in enumerate(BIRADS)}\n",
    "\n",
    "word_to_idx_density_mammo={word:idx for idx,word in enumerate(DENSITY_MAMMO)}\n",
    "idx_to_word_density_mammo={idx:word for idx,word in enumerate(DENSITY_MAMMO)}\n",
    "\n",
    "word_to_idx_calcifications_benign={word:idx for idx,word in enumerate(CALCIFICATIONS_BENIGN)}\n",
    "idx_to_word_calcifications_benign={idx:word for idx,word in enumerate(CALCIFICATIONS_BENIGN)}\n",
    "\n",
    "word_to_idx_ganglio_mamo={word:idx for idx,word in enumerate(GANGLIO_MAMO)}\n",
    "idx_to_word_ganglio_mamo={idx:word for idx,word in enumerate(GANGLIO_MAMO)}\n",
    "\n",
    "word_to_idx_density_echo={word:idx for idx,word in enumerate(DENSITY_ECHO)}\n",
    "idx_to_word_density_echo={idx:word for idx,word in enumerate(DENSITY_ECHO)}\n",
    "\n",
    "word_to_idx_lymph_benign={word:idx for idx,word in enumerate(LYMPH_BENIGN)}\n",
    "idx_to_word_lymph_benign={idx:word for idx,word in enumerate(LYMPH_BENIGN)}\n",
    "\n",
    "word_to_idx_lymph_suspicious={word:idx for idx,word in enumerate(LYMPH_SUSPICIOUS)}\n",
    "idx_to_word_lymph_suspicious={idx:word for idx,word in enumerate(LYMPH_SUSPICIOUS)}\n",
    "\n",
    "word_to_idx_simple_cyst={word:idx for idx,word in enumerate(SIMPLE_CYST)}\n",
    "idx_to_word_simple_cyst={idx:word for idx,word in enumerate(SIMPLE_CYST)}\n",
    "\n",
    "word_to_idx_ductal_ectasia={word:idx for idx,word in enumerate(DUCTAL_ECTASIA)}\n",
    "idx_to_word_ductal_ectasia={idx:word for idx,word in enumerate(DUCTAL_ECTASIA)}\n",
    "DICTIONARY={\"tipo\":TIPO,\"tecnica\":TECNICA,\"family\":FAMILY,\"prosthesis\":PROSTHESIS,\"birads\":BIRADS,\"density_mammo\":DENSITY_MAMMO,\"calcifications_benign\":CALCIFICATIONS_BENIGN,\n",
    "            \"ganglio_mamo\":GANGLIO_MAMO,\"density_echo\":DENSITY_ECHO,\"lymph_benign\":LYMPH_BENIGN,\"lymph_suspicious\":LYMPH_SUSPICIOUS,\"simple_cyst\":SIMPLE_CYST,\"ductal_ectasia\":DUCTAL_ECTASIA,\n",
    "           \"nodules_echo\": NODULES_ECHO,\"nodules_shape\":NODULES_SHAPE,\"nodules_margin\":NODULES_MARGIN, \"nodules_echogenicity\":NODULES_ECHOGENICITY, \"nodules_known\":NODULES_KNOWN, \"nodules_stable\":NODULES_STABLE}\n",
    "\n",
    "\n",
    "outputs=[]\n",
    "for tipo in DICTIONARY.values():\n",
    "    outputs+=tipo\n",
    "print(outputs)\n",
    "\n",
    "word_to_idx_out={word:idx for idx,word in enumerate(outputs)}\n",
    "idx_to_word_out={idx:word for idx,word in enumerate(outputs)}\n",
    "import gc\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def fix_brackets_spaces(texto):\n",
    "    ''' \n",
    "        Introduce espacios por delante y por detr√°s de los par√©ntesis.\n",
    "        Esta medida mejora el tokenizado de Spacy\n",
    "    '''\n",
    "    \n",
    "    texto = re.sub(r'([(\\[¬ø!])', r' \\1', texto)\n",
    "    texto = re.sub(r'([)\\]?¬°])', r'\\1 ', texto)\n",
    "                \n",
    "    return texto\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    text= fix_brackets_spaces(text)\n",
    "    # print(text)\n",
    "    \n",
    "    # print(text)\n",
    "    return text\n",
    "\n",
    "    \n",
    "def flatten_and_filter_dataset(ground_truth,reports):\n",
    "    \"\"\"\n",
    "    Esta funci√≥n toma un conjunto de datos en el formato original (con estructura jer√°rquica)\n",
    "    y devuelve un conjunto de datos plano, donde cada entrada tiene un solo `context`, `question` y `answer`.\n",
    "    \n",
    "    Argumentos:\n",
    "        dataset: Un conjunto de datos en formato original (puede ser train, validation, test).\n",
    "    \n",
    "    Retorno:\n",
    "        Un conjunto de datos de Hugging Face en formato plano, con solo ejemplos completos.\n",
    "    \"\"\"\n",
    "    # Lista para almacenar ejemplos en formato plano\n",
    "    \n",
    "    j=0\n",
    "    flattened_examples = {}\n",
    "    examples_raw={}\n",
    "    targets={}\n",
    "    val_data={}\n",
    "    question_tipo={}\n",
    "    previous_message_answer_tipo={}\n",
    "    options_tipo={}\n",
    "    answers_tipo={}\n",
    "    j=0\n",
    "    \n",
    "    question_tipo[\"tipo\"]= 'is the following breast medical report a biopsy report or a nodal staging ultrasound report?'\n",
    "    previous_message_answer_tipo[\"tipo\"]=\"biopsy reports are Image-Guided Biopsy or Fine needle aspiration and is normally said that they are referred to the hospital for biopsy. Nodal staging ultrasound reports can also be written as 'axilla ultrasound'. If it is any of these it will be written in the beginning of the report, normally in the used technique. These kind of reports are only ultrasound.\"\n",
    "    \n",
    "\n",
    "    question_tipo[\"tecnica\"]= 'what diagnostic technique was used in the following breast medical report?'\n",
    "    previous_message_answer_tipo[\"tecnica\"]=\"biopsy reports, simple cysts and analysis of lymph or axillary nodes are only seen on ultrasound. On the other hand, if the ACR density is given or parenchymal distortions are analysed, the technique will be a mammogram. Tomosyntesis is a mammography type. The report may include an ultrasound examination, a mammography examination or both.\"\n",
    "    \n",
    "\n",
    "    question_tipo[\"family\"]= \"does the patient have any family history in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"family\"]=\"family history of breast cancer is categorized based on the degree of relatives affected: First-degree relatives: Parents, siblings, or children. Second-degree relatives: Grandparents, aunts, uncles, nieces, nephews, or half-siblings. Third-degree relatives: Great-grandparents, great-aunts/uncles, or first cousins.\"\n",
    "    \n",
    "    question_tipo[\"prosthesis\"]= \"does the patient have a prosthesis in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"prosthesis\"]=\"it is normally clearly indicated at the beginning of the report. Sometimes it is written as implants instead of prosthesis.\"\n",
    "    \n",
    "    question_tipo[\"birads\"]= \"what is the final BI-RADS classification given to the patient in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"birads\"]=\"the final BI-RADS of the patient is given in the conclusions of the report, normally at the end.\"\n",
    "\n",
    "    question_tipo[\"density_mammo\"]= \"what is the breast density found in the mammography study of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"density_mammo\"]=\"breast density in mammography is classified into four categories: ACR A (= Almost entirely fatty), ACR B (= Scattered areas of fibroglandular density), ACR C (= Heterogeneously dense), ACR D (= Extremely or very dense breasts). Sometimes it is written as 'density type x' or with their real meaning (very dense breasts = C).\"\n",
    "       \n",
    "    question_tipo[\"density_echo\"]= \"what is the breast density found in the ultrasound study of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"density_echo\"]=\"breast composition in ultrasound is classified into four categories: fibroglandular and fat (mixed distribution of fibroglandular and adipose tissue), heterogeneous fibroglandular (predominantly fibroglandular tissue with varying echogenicity and scattered fat areas), homogeneous fatty (uniform fatty tissue with consistent echogenicity and minimal fibroglandular content), and homogeneous fibroglandular (uniform fibroglandular tissue with consistent echogenicity and minimal fat content).\"\n",
    "    \n",
    "    question_tipo[\"calcifications_benign\"]= \"does the following breast medical report mention the appearence of benign calcifications in the mammography exam?\"\n",
    "    previous_message_answer_tipo[\"calcifications_benign\"]=\"Consider only benign calcifications in the mammography.\"\n",
    "      \n",
    "    question_tipo[\"ganglio_mamo\"]= \"does the following breast medical report mention any lymph nodes in the mammography exam?\"\n",
    "    previous_message_answer_tipo[\"ganglio_mamo\"]=\"Consider only lymph nodes that appear in the mammography.\"\n",
    "    \n",
    "    question_tipo[\"lymph_suspicious\"]= \"does the following breast medical report mention any suspicious axillary lymph nodes in the ultrasound exam?\"\n",
    "    previous_message_answer_tipo[\"lymph_suspicious\"]=\"if a lymph node is suspicious the report will recomend a biopsy or Fine Needle Aspiration. A lymph node is considered suspicious when it has eccentric cortical thickening ‚â• 3 mm, Short axis >10 mm, round shape, loss of fatty hilum, abnormal vascularity, or irregular margins‚Äîespecially when associated with known malignancy or progressive enlargement. They can also be classified as UN3, UN4 or UN5. An exam may have both suspicious and benign lymph nodes.\"\n",
    "\n",
    "    question_tipo[\"lymph_benign\"]= \"does the following breast medical report mention any benign or not suspicious axillary lymph nodes in the ultrasound exam?\"\n",
    "    previous_message_answer_tipo[\"lymph_benign\"]=\"if a lymph node is suspicious the report will recomend a biopsy or Fine Needle Aspiration. A lymph node is considered benign when it has uniform cortex < 3 mm, preserved fatty hilum, oval shape, no abnormal vascularity, no irregular margins and homogeneous internal echo pattern. Benign axillary nodes can be classified as UN1 or UN2. A reactive axillary node is not suspicious. An exam may have both suspicious and benign lymph nodes.\"\n",
    "    \n",
    "    question_tipo[\"simple_cyst\"]= \"does the following breast medical report mention any simple cysts or microcysts in the ultrasound exam?\"\n",
    "    previous_message_answer_tipo[\"simple_cyst\"]=\"The words symple cysts or microcysts will appear only in the ultrasound exam. Sometimes they can say that some of the cysts have echogenic content, but we still will consider them simple cysts and not nodules.\"\n",
    "    \n",
    "    question_tipo[\"ductal_ectasia\"]= \"does the following breast medical report mention any ductal ectasia in the ultrasound exam?\"\n",
    "    previous_message_answer_tipo[\"ductal_ectasia\"]=\"The word ductal ectasia will appear only in the ultrasound exam.\"\n",
    "\n",
    "    question_tipo[\"nodules_echo\"]= \"is there any nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_echo\"]=\"The localization, echogenicity and size of the nodules are normally said.\"\n",
    "    \n",
    "    question_tipo[\"nodules_shape\"]= \"what is the shape of the first nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_shape\"]=\"Shapes can be 'oval', 'round', 'lobulated' and 'irregular'. Sometimes irregular is also used for the margin, but in this case it is written as 'irregular margin' or 'irregular borders'. \"\n",
    "    \n",
    "    question_tipo[\"nodules_margin\"]= \"what is the margin of the first nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_margin\"]=\"Margin can be 'circumscribed' and 'not circumscribed'. Inside the not circumscribed we have 'spiculated', 'angulated', 'microlobulated' or 'indistinc' ('not defined') margins. Sometimes irregular is also used for the margin, but in this case it is written as 'irregular margin' or 'irregular borders', in this case classify it as 'not circumscribed'.\"\n",
    "    \n",
    "    question_tipo[\"nodules_echogenicity\"]= \"what is the echogenicity of the first nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_echogenicity\"]=\"Echogenicity can be 'anechoic', 'hypoechoic', 'heterogeneous' and 'complex cystic and solid'.\" \n",
    "    \n",
    "    question_tipo[\"nodules_known\"]= \"is the first nodule described in the ultrasound exam of the following breast medical report previously known?\"\n",
    "    previous_message_answer_tipo[\"nodules_known\"]=\"If the nodule is known from before the report, it will say if it it is stable or if it has grown or shrink.\"\n",
    "    \n",
    "    question_tipo[\"nodules_stable\"]= \"is the first known nodule described in the ultrasound exam stable in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_stable\"]=\"If the nodule is known from before the examination, it will be analysed to see if it is stable or if it got bigger or smaller. \"\n",
    "    \n",
    "    for i, report in enumerate(reports[\"informes_ingles\"]):\n",
    "        informe=preprocess_text(report)\n",
    "        key=reports[\"keys\"][i]\n",
    "        \n",
    "        if key not in ground_truth.index:\n",
    "            continue\n",
    "        if key in flattened_examples:\n",
    "            continue\n",
    "\n",
    "        n_tipo=np.zeros(len(outputs))\n",
    "        n_tecnica=np.zeros(len(outputs))\n",
    "        n_family=np.zeros(len(outputs))\n",
    "        n_prosthesis=np.zeros(len(outputs))\n",
    "        n_birads=np.zeros(len(outputs))\n",
    "        n_density_mammo=np.zeros(len(outputs))\n",
    "        n_calcifications_benign=np.zeros(len(outputs))\n",
    "        n_ganglio_mamo=np.zeros(len(outputs))\n",
    "        n_density_echo=np.zeros(len(outputs))\n",
    "        n_lymph_benign=np.zeros(len(outputs))\n",
    "        n_lymph_suspicious=np.zeros(len(outputs))\n",
    "        n_simple_cyst=np.zeros(len(outputs))\n",
    "        n_ductal_ectasia=np.zeros(len(outputs))\n",
    "        n_nodules_echo=np.zeros(len(outputs))\n",
    "        n_nodules_shape=np.zeros(len(outputs))\n",
    "        n_nodules_margin=np.zeros(len(outputs))\n",
    "        n_nodules_echogenicity=np.zeros(len(outputs))\n",
    "        n_nodules_known=np.zeros(len(outputs))\n",
    "        n_nodules_stable=np.zeros(len(outputs))\n",
    "        row=ground_truth.loc[key]\n",
    "        answer_tipo={}\n",
    "        #TIPO\n",
    "        normal_control=False\n",
    "        if row[\"Biopsy_report\"]==\"Yes\":\n",
    "            n_tipo[word_to_idx_out[\"biopsy report\"]]=1\n",
    "            \n",
    "        elif row[\"Ganglio_report\"]==\"Yes\":\n",
    "            n_tipo[word_to_idx_out[\"nodal staging ultrasound report\"]]=1\n",
    "        else:\n",
    "            normal_control=True\n",
    "            n_tipo[word_to_idx_out[\"normal control or revision report\"]]=1\n",
    "        answer_tipo[\"tipo\"]=n_tipo\n",
    "        #TECHNIQUE\n",
    "        tecnica=row[\"Technique\"]\n",
    "        # Verificar si el ejemplo tiene preguntas\n",
    "        if tecnica==\"ultrasound\":\n",
    "            n_tecnica[word_to_idx_out[\"only ultrasound study\"]]=1          \n",
    "        elif tecnica==\"mammography\":\n",
    "            n_tecnica[word_to_idx_out[\"only mammography study\"]]=1\n",
    "        elif not pd.isna(tecnica):\n",
    "            n_tecnica[word_to_idx_out[tecnica]]=1\n",
    "        else:\n",
    "            print(key,report)\n",
    "        answer_tipo[\"tecnica\"]=n_tecnica\n",
    "        # \n",
    "        # HISTORY\n",
    "        #No consideramos las biopsias o las ecograf√≠as de estadificaci√≥n ganglionar.\n",
    "        if normal_control:\n",
    "            \n",
    "            family=row[\"Family_history\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(family,str) or family==\"No\":\n",
    "                n_family[word_to_idx_out[\"no family history\"]]=1         \n",
    "            else:\n",
    "                n_family[word_to_idx_out[family]]=1\n",
    "            answer_tipo[\"family\"]=n_family    \n",
    "            # PROSTHESIS\n",
    "            prosthesis=row[\"Prosthesis\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(prosthesis,str) or prosthesis==\"No\":\n",
    "                n_prosthesis[word_to_idx_out[\"no prosthesis\"]]=1        \n",
    "            else:\n",
    "                n_prosthesis[word_to_idx_out[\"yes prosthesis\"]]=1\n",
    "            answer_tipo[\"prosthesis\"]=n_prosthesis\n",
    "            #BIRADS\n",
    "            birads=row[\"BI-RADS\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(birads,str):\n",
    "                n_birads[word_to_idx_out[\"unknown BI-RADS\"]]=1           \n",
    "            else:\n",
    "                n_birads[word_to_idx_out[birads]]=1\n",
    "            answer_tipo[\"birads\"]=n_birads\n",
    "            #Density mammo\n",
    "            density_mammo=row[\"Density_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_mammo,str) or density_mammo not in DENSITY_MAMMO:\n",
    "                n_density_mammo[word_to_idx_out[\"unknown density mammo\"]]=1       \n",
    "            else:\n",
    "                n_density_mammo[word_to_idx_out[density_mammo]]=1\n",
    "            answer_tipo[\"density_mammo\"]=n_density_mammo\n",
    "            #Lymp nodes mammo\n",
    "            ganglio_mamo=row[\"Ganglio_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ganglio_mamo,str):\n",
    "                n_ganglio_mamo[word_to_idx_out[\"no ganglio\"]]=1            \n",
    "            else:\n",
    "                n_ganglio_mamo[word_to_idx_out[ganglio_mamo.lower()+\" ganglio\"]]=1\n",
    "            answer_tipo[\"ganglio_mamo\"]=n_ganglio_mamo\n",
    "            #Calcifications benign\n",
    "            calcifications_benign=row[\"Calcifications_benign_mamo\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(calcifications_benign,str):\n",
    "                n_calcifications_benign[word_to_idx_out[\"no calcifications\"]]=1       \n",
    "            else:\n",
    "                n_calcifications_benign[word_to_idx_out[calcifications_benign.lower()+ \" calcifications\"]]=1\n",
    "            answer_tipo[\"calcifications_benign\"]=n_calcifications_benign\n",
    "        \n",
    "    \n",
    "            #Density echo\n",
    "            density_echo=row[\"Density_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(density_echo,str)or density_echo not in DENSITY_ECHO:\n",
    "                n_density_echo[word_to_idx_out[\"unknown density echo\"]]=1         \n",
    "            else:\n",
    "                n_density_echo[word_to_idx_out[density_echo]]=1\n",
    "            answer_tipo[\"density_echo\"]=n_density_echo\n",
    "            #Benign lymph nodes\n",
    "            simple_cyst=row[\"simple_cyst_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(simple_cyst,str):\n",
    "                n_simple_cyst[word_to_idx_out[\"no cyst\"]]=1         \n",
    "            else:\n",
    "                n_simple_cyst[word_to_idx_out[simple_cyst.lower()+\" cyst\"]]=1\n",
    "\n",
    "            answer_tipo[\"simple_cyst\"]=n_simple_cyst\n",
    "            #Suspicious lymph nodes\n",
    "            lymph_suspicious=row[\"Ganglio_suspicious_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(lymph_suspicious,str):\n",
    "                n_lymph_suspicious[word_to_idx_out[\"no lymph suspicious\"]]=1         \n",
    "            else:\n",
    "                n_lymph_suspicious[word_to_idx_out[lymph_suspicious.lower()+ \" lymph suspicious\"]]=1\n",
    "            answer_tipo[\"lymph_suspicious\"]=n_lymph_suspicious\n",
    "            #Benign lymph nodes\n",
    "            lymph_benign=row[\"Ganglio_benign_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            \n",
    "            if not isinstance(lymph_benign,str):\n",
    "                n_lymph_benign[word_to_idx_out[\"no lymph benign\"]]=1           \n",
    "            else:\n",
    "                n_lymph_benign[word_to_idx_out[lymph_benign.lower()+ \" lymph benign\"]]=1\n",
    "            answer_tipo[\"lymph_benign\"]=n_lymph_benign\n",
    "            #Ductal ectasia\n",
    "            ductal_ectasia=row[\"Ductal_ectasia_eco\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(ductal_ectasia,str):\n",
    "                n_ductal_ectasia[word_to_idx_out[\"no ectasia\"]]=1    \n",
    "            else:\n",
    "                n_ductal_ectasia[word_to_idx_out[ductal_ectasia.lower()+\" ectasia\"]]=1\n",
    "            answer_tipo[\"ductal_ectasia\"]=n_ductal_ectasia\n",
    "\n",
    "            nodules_echo=row[\"Nodules_eco\"]\n",
    "            nodules_bool=False\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(nodules_echo,str) and not isinstance(nodules_echo,int):\n",
    "                n_nodules_echo[word_to_idx_out[\"no nodules\"]]=1\n",
    "            elif isinstance(nodules_echo,str) and nodules_echo==\"No\":\n",
    "                n_nodules_echo[word_to_idx_out[\"no nodules\"]]=1\n",
    "            else:\n",
    "                nodules_bool=True\n",
    "                n_nodules_echo[word_to_idx_out[\"yes nodules\"]]=1\n",
    "            answer_tipo[\"nodules_echo\"]=n_nodules_echo\n",
    "            if nodules_bool:\n",
    "                #Density echo\n",
    "                nodules_shape=row[\"Shape_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_shape,str)or nodules_shape not in NODULES_SHAPE:\n",
    "                    n_nodules_shape[word_to_idx_out[\"unknown shape\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_shape[word_to_idx_out[nodules_shape]]=1\n",
    "                answer_tipo[\"nodules_shape\"]=n_nodules_shape\n",
    "\n",
    "                nodules_margin=row[\"Margin_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_margin,str)or nodules_margin not in NODULES_MARGIN:\n",
    "                    n_nodules_margin[word_to_idx_out[\"unknown margin\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_margin[word_to_idx_out[nodules_margin]]=1\n",
    "                answer_tipo[\"nodules_margin\"]=n_nodules_margin\n",
    "\n",
    "                nodules_echogenicity=row[\"Echogenicity_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_echogenicity,str)or nodules_echogenicity not in NODULES_ECHOGENICITY:\n",
    "                    n_nodules_echogenicity[word_to_idx_out[\"unknown echogenicity\"]]=1         \n",
    "                else:\n",
    "                    n_nodules_echogenicity[word_to_idx_out[nodules_echogenicity]]=1\n",
    "                answer_tipo[\"nodules_echogenicity\"]=n_nodules_echogenicity\n",
    "\n",
    "                #Nodules echo known\n",
    "                nodules_known=row[\"new_eco_1\"]\n",
    "                known_bool=False\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_known,str):\n",
    "                    n_nodules_known[word_to_idx_out[\"unknown known\"]]=1\n",
    "                elif nodules_known==\"No\":\n",
    "                    known_bool=True\n",
    "                    n_nodules_known[word_to_idx_out[\"yes known\"]]=1    \n",
    "                else:\n",
    "                    n_nodules_known[word_to_idx_out[\"no known\"]]=1\n",
    "                answer_tipo[\"nodules_known\"]=n_nodules_known\n",
    "                if known_bool:\n",
    "                    #Nodules echo stable\n",
    "                    nodules_stable=row[\"Stable_eco_1\"]\n",
    "                    # Verificar si el ejemplo tiene preguntas\n",
    "                    if not isinstance(nodules_stable,str):\n",
    "                        n_nodules_stable[word_to_idx_out[\"unknown stable\"]]=1\n",
    "                    else:\n",
    "                        n_nodules_stable[word_to_idx_out[nodules_stable.lower()+\" stable\"]]=1\n",
    "                    answer_tipo[\"nodules_stable\"]=n_nodules_stable\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "        for tipo in answer_tipo:\n",
    "            #Si est√° el tipo en las respuestas que hemos recogido lo metemos a la base de datos.\n",
    "            key_tipo=key+\"_\"+tipo\n",
    "            if key_tipo in flattened_examples:\n",
    "                continue\n",
    "                key_tipo=key_tipo+\"_copy\"\n",
    "            examples_raw[key_tipo]=report\n",
    "            answer=answer_tipo[tipo]\n",
    "            \n",
    "    \n",
    "            inputs_tipo = \"Question: \" + question_tipo[tipo] +\" Extra information: \"+ previous_message_answer_tipo[tipo]+ \" Context: \" + informe\n",
    "            flattened_examples[key_tipo]=inputs_tipo\n",
    "            targets[key_tipo]=int(np.argmax(answer))\n",
    "    return flattened_examples,targets\n",
    "inputs,targets = flatten_and_filter_dataset(ground_truth,report_data)   \n",
    "\n",
    "dataset_final=pd.DataFrame.from_dict(inputs,orient='index')\n",
    "targets=pd.DataFrame.from_dict(targets,orient='index')\n",
    "dataset_final.columns=[\"text\"]\n",
    "targets.columns=[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "def visualize_errors(valid_dataset,valid_targets,validation_predictions,keys):\n",
    "    # Crear un DataFrame con los textos originales, las etiquetas reales y las predicciones\n",
    "    results_df = pd.DataFrame({\n",
    "        'key':list(keys),\n",
    "        'Text': list(valid_dataset),  # Usamos los textos originales\n",
    "        'True Label': list(valid_targets),\n",
    "        'Predicted Label': list(validation_predictions)\n",
    "    })\n",
    "    \n",
    "    # Filtrar los ejemplos en los que el modelo fall√≥\n",
    "    errors_df = results_df[results_df['True Label'] != results_df['Predicted Label']]\n",
    "    \n",
    "    for ind,row in errors_df.iterrows():\n",
    "        print(row[\"key\"])\n",
    "        print(\"EJEMPLO\")\n",
    "        print(row[\"Text\"])\n",
    "        print(\"PREDICTED\")\n",
    "        print(idx_to_word_out[row[\"Predicted Label\"]])\n",
    "        print(\"TRUE\")\n",
    "        print(idx_to_word_out[row[\"True Label\"]])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    texts = examples[\"text\"]\n",
    "    \n",
    "    outputs = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Verificar truncaci√≥n\n",
    "    for i, text in enumerate(texts):\n",
    "        untruncated = tokenizer(\n",
    "            text,\n",
    "            truncation=False,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        if len(untruncated[\"input_ids\"]) > 512:\n",
    "            print(\"‚ö†Ô∏è Truncation occurred!\")\n",
    "            print(f\"Original length: {len(untruncated['input_ids'])}, Truncated to: 512\")\n",
    "            print(\"Sample text:\", text[:200], \"...\\n\")\n",
    "\n",
    "    return outputs\n",
    "def test_train_split(i, X, Y):\n",
    "    random.seed(1)\n",
    "    # Agrupar ejemplos originales y sus copias\n",
    "    def obtener_grupo(nombre):\n",
    "        # Extraer el nombre base eliminando '_copia' y cualquier n√∫mero posterior\n",
    "        if '_copy' in nombre:\n",
    "            return nombre.split('_copy')[0]\n",
    "\n",
    "        return nombre\n",
    "\n",
    "    # Crear un DataFrame temporal para manejar los √≠ndices\n",
    "    agrupaciones = pd.DataFrame(index=X.index)\n",
    "    agrupaciones['grupo'] = agrupaciones.index.map(obtener_grupo)\n",
    "    # Obtener una lista de grupos √∫nicos\n",
    "    grupos_unicos = agrupaciones['grupo'].unique()\n",
    "\n",
    "    # Dividir grupos en 10 folds\n",
    "    cv_tama√±o = math.ceil(len(grupos_unicos) / 10)\n",
    "    test_grupos = grupos_unicos[i * cv_tama√±o:min(len(grupos_unicos), (i + 1) * cv_tama√±o)]\n",
    "    \n",
    "\n",
    "    # Filtrar conjuntos de prueba y entrenamiento basados en los grupos\n",
    "    test_indices = test_grupos\n",
    "    train_indices = agrupaciones[~agrupaciones['grupo'].isin(test_grupos)].index\n",
    "    \n",
    "    # Seleccionar los datos de entrenamiento y prueba\n",
    "    test = X.loc[test_indices].sort_index()\n",
    "    test_y = Y.loc[test_indices].sort_index()\n",
    "    ind=list(test.index)\n",
    "    \n",
    "    train = X.loc[train_indices].sort_index()\n",
    "\n",
    "    # Barajar los datos de entrenamiento para evitar sesgos\n",
    "    train = train.sample(frac=1, random_state=1)\n",
    "    print(train)\n",
    "    train_y = Y.loc[train.index]\n",
    "    print(train_y)\n",
    "    train[\"label\"]=train_y[\"label\"]\n",
    "    test[\"label\"]=test_y[\"label\"]\n",
    "    # Devolver los conjuntos\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    return train, test, ind\n",
    "\n",
    "def evaluate_per_question(predicted, tested, DICTIONARY):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions per question type.\n",
    "    \n",
    "    Parameters:\n",
    "    - predicted: array of predicted label indices (flattened from all folds)\n",
    "    - tested: array of true label indices (same shape as predicted)\n",
    "    - DICTIONARY: dict mapping each question to its list of class names\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Build global index ‚Üí (question, class_name) mapping\n",
    "    idx_to_question_value = {}\n",
    "    offset = 0\n",
    "    question_offsets = {}\n",
    "    for question, class_list in DICTIONARY.items():\n",
    "        question_offsets[question] = offset\n",
    "        for i, label in enumerate(class_list):\n",
    "            idx_to_question_value[offset + i] = (question, label)\n",
    "        offset += len(class_list)\n",
    "\n",
    "    # Step 2: Group predictions by question\n",
    "    per_question_true = defaultdict(list)\n",
    "    per_question_pred = defaultdict(list)\n",
    "\n",
    "    for true_idx, pred_idx in zip(tested, predicted):\n",
    "        q_true, _ = idx_to_question_value[true_idx]\n",
    "        # You can check if q_true == q_pred here for safety if needed\n",
    "        per_question_true[q_true].append(true_idx)\n",
    "        per_question_pred[q_true].append(pred_idx)\n",
    "\n",
    "    # Step 3: Classification reports\n",
    "    print(\"\\nüîç Per-question classification reports:\\n\")\n",
    "    for question, true_labels in per_question_true.items():\n",
    "        pred_labels = per_question_pred[question]\n",
    "        label_names = DICTIONARY[question]\n",
    "        start = question_offsets[question]\n",
    "        end = start + len(label_names)\n",
    "        question_label_ids = list(range(start, end))\n",
    "\n",
    "        print(f\"\\nüìò Question: {question}\")\n",
    "        try:\n",
    "            print(classification_report(true_labels, pred_labels, labels=question_label_ids, target_names=label_names))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not generate report for '{question}': {e}\")\n",
    "\n",
    "    print(\"\\nüìä Accuracy per class and per question:\\n\")\n",
    "    for question in DICTIONARY:\n",
    "        y_true = np.array(per_question_true[question])\n",
    "        y_pred = np.array(per_question_pred[question])\n",
    "        class_list = DICTIONARY[question]\n",
    "        start = question_offsets[question]\n",
    "    \n",
    "        if len(y_true) == 0:\n",
    "            print(f\"\\n‚ùå {question}: [No data]\")\n",
    "            continue\n",
    "    \n",
    "        print(f\"\\n‚úÖ Accuracy for: {question}\")\n",
    "        # Per-class accuracy\n",
    "        for i, class_name in enumerate(class_list):\n",
    "            global_idx = start + i\n",
    "            mask = y_true == global_idx\n",
    "            if mask.sum() == 0:\n",
    "                print(f\"  {class_name}: [No samples]\")\n",
    "                continue\n",
    "            acc = accuracy_score(y_true[mask], y_pred[mask])\n",
    "            print(f\"  {class_name}: {acc:.4f}\")\n",
    "        \n",
    "        # Overall accuracy for the question\n",
    "        overall_acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"üéØ Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation(X,Y):\n",
    "    \n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    predicted=[]\n",
    "    tested=[]\n",
    "    acc_cv=[]\n",
    "    kappa_cv=[]\n",
    "    ind_cv=[]\n",
    "    for i in range(10):\n",
    "        print(\"EPOCH \",i)\n",
    "        train, test, ind = test_train_split(i,X,Y)\n",
    "        # Diferenciamos el fit cuando el resultado es categorical o no.\n",
    "        print(len(train),len(test))\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=64 # Cambia seg√∫n tus clases\n",
    "        )\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        train_data = Dataset.from_pandas(train)\n",
    "        test_data=Dataset.from_pandas(test)\n",
    "        train_data = train_data.map(tokenize_function, batched=True)\n",
    "        train_data = train_data.rename_column(\"label\", \"labels\")\n",
    "        train_data = train_data.remove_columns([\"text\"])\n",
    "        train_data.set_format(\"torch\")\n",
    "        test_data = test_data.map(tokenize_function, batched=True)\n",
    "        test_data = test_data.rename_column(\"label\", \"labels\")\n",
    "        test_data = test_data.remove_columns([\"text\"])\n",
    "        test_data.set_format(\"torch\")\n",
    "        print(train_data)\n",
    "        print(test_data)\n",
    "        if 'token_type_ids' in train_data:\n",
    "            train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "            test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "        print(\"Label value check:\")\n",
    "        print(\"Train labels unique:\", sorted(train[\"label\"].unique()))\n",
    "        print(\"Test labels unique:\", sorted(test[\"label\"].unique()))\n",
    "        print(\"Max label:\", max(train[\"label\"].max(), test[\"label\"].max()))\n",
    "        print(\"Min label:\", min(train[\"label\"].min(), test[\"label\"].min()))\n",
    "        trainer = Trainer(\n",
    "        model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=test_data  # Si tienes datos de validaci√≥n, √∫salos aqu√≠\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        trainer = Trainer(\n",
    "        model=model,\n",
    "            args=training_args_all,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=test_data  # Si tienes datos de validaci√≥n, √∫salos aqu√≠\n",
    "        )\n",
    "        trainer.train()\n",
    "        pred = trainer.predict(test_data)\n",
    "        outputs=pred.predictions.argmax(axis=-1)\n",
    "        print(outputs)\n",
    "        accuracy = accuracy_score(test[\"label\"], outputs)\n",
    "        acc_cv.append(accuracy)\n",
    "        ind_cv.append(ind)\n",
    "        predicted.append(outputs)\n",
    "        tested.append(test[\"label\"].values)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(test[\"label\"], outputs))\n",
    "        evaluate_per_question(outputs, test[\"label\"], DICTIONARY)\n",
    "        # valid_dataset=[report for key,report in examples_raw.items() if key in ind]\n",
    "        # visualize_errors(valid_dataset,np.array(test[\"label\"]).squeeze(),outputs,ind)\n",
    "    predicted=np.concatenate(predicted)\n",
    "    ind_cv=np.concatenate(ind_cv)\n",
    "    tested=np.concatenate(tested)\n",
    "    acc_final=accuracy_score(tested, predicted)\n",
    "    acc_std=np.std(acc_cv)\n",
    "    evaluate_per_question(predicted, tested, DICTIONARY)\n",
    "    \n",
    "    return predicted,tested"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
