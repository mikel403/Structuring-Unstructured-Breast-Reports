{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39df26-137d-4c5d-93cb-8d89499143d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from bert_score import score as bert_score_fn\n",
    "import pickle\n",
    "import evaluate\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1875866-6420-4c49-bb07-beb23b9b4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_present_dic={\"age\": \"no.\",\"history\":\"no history was found.\",\"parenchymal_distortion\":\"no.\",\"nodules_echo_size\":\"unknown.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e1180-456b-43ac-a0d7-e197dd58635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"biogptQA2e55e66no_copydescription8batchfinal\"\n",
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "# questions=[\"age\",\"tipo\",\"tecnica\",\"family\",\"history\",\"symtomatic\",\n",
    "#            \"prosthesis\",\"birads\",\"density_mammo\",\"calcifications_benign\",\"density_echo\",\"ganglio_mamo\",\"lymph_benign\",\"lymph_suspicious\",\"parenchymal_distortion\",\"simple_cyst\",\"ductal_ectasia\",\n",
    "#           \"nodules_echo_num\",\"nodules_echo_description\",\"nodules_echo_shape\",\"nodules_echo_margin\",\"nodules_echo_echogenicity\",\"nodules_echo_location\",\"nodules_echo_size\",\"nodules_echo_known\",\"nodules_echo_stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dce470-ed1f-4626-a808-fde0b93f34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_before_keyword(text, keyword):\n",
    "    \"\"\"Truncate text before keyword (e.g., 'comparative study') and end with a period.\"\"\"\n",
    "    keyword_pos = text.lower().find(keyword.lower())\n",
    "    if keyword_pos == -1:\n",
    "        cleaned = text.strip()\n",
    "    else:\n",
    "        # Text before keyword\n",
    "        partial = text[:keyword_pos]\n",
    "        # Find last punctuation before keyword\n",
    "        match = re.search(r'(.+?)[\\.\\,\\;\\n][^.,;\\n]*$', partial)\n",
    "        cleaned = match.group(1).strip() if match else partial.strip()\n",
    "    \n",
    "    # Ensure the result ends with a single period\n",
    "    if not cleaned.endswith('.'):\n",
    "        cleaned += '.'\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cc2ae-b72e-4f9c-8ec5-078c1bf19cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(pred,truth,tipo,point=False):\n",
    "    acc=0\n",
    "    total=0\n",
    "    for key,real in truth.items():\n",
    "        if key in pred:\n",
    "            total+=1\n",
    "            predicted=pred[key]   \n",
    "            \n",
    "            if predicted.startswith(not_present_dic[tipo]):\n",
    "                predicted=\"no response.\"\n",
    "            elif tipo==\"nodules_echo_size\":\n",
    "                predicted = predicted.split(\"mm\")[0] + \"mm.\"\n",
    "            else:\n",
    "                predicted = predicted.split(\".\")[0] + \".\"\n",
    "            predicted=truncate_before_keyword(predicted, \"comparative study\")\n",
    "            if predicted.lower()==str(real).lower():\n",
    "                acc+=1\n",
    "            # else:\n",
    "            #     print(key,\"REAL: \", real,\"PREDICTED: \",predicted)\n",
    "        else:\n",
    "            print(key)\n",
    "    print(acc)\n",
    "    print(total)\n",
    "\n",
    "    return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede74ef-eb94-4499-996c-781feb2cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def calculate_metrics(pred, truth, tipo):\n",
    "    preds, refs = [], []\n",
    "    for key, real in truth.items():\n",
    "        if key not in pred:\n",
    "            continue\n",
    "        predicted = pred[key]\n",
    "        if tipo in not_present_dic:\n",
    "            if predicted.startswith(not_present_dic[tipo]):\n",
    "                predicted=\"no response.\"\n",
    "            elif tipo==\"nodules_echo_size\":\n",
    "                predicted = predicted.split(\"mm\")[0] + \"mm.\"\n",
    "            else:\n",
    "                predicted = predicted.split(\".\")[0] + \".\"\n",
    "        \n",
    "        predicted=truncate_before_keyword(predicted, \"comparative study\")\n",
    "        preds.append(predicted)\n",
    "        refs.append(real)\n",
    "    # Compute metrics\n",
    "    f1s, ems = [], []\n",
    "    for p, r in zip(preds, refs):\n",
    "        f1s.append(f1_score(p, r))\n",
    "        ems.append(int(normalize_answer(p) == normalize_answer(r)))\n",
    "\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    P, R, F1 = bert_score_fn(preds, refs, lang=\"en\", verbose=False, rescale_with_baseline=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bert_R = R.mean().item()\n",
    "\n",
    "    print(f\"Examples: {len(preds)}\")\n",
    "    print(f\"Token F1: {sum(f1s)/len(f1s):.4f}\")\n",
    "    print(f\"Exact Match: {sum(ems)/len(ems):.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_f1:.4f}\")\n",
    "    print(f\"BERTScore R: {bert_R:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"token_f1\": sum(f1s)/len(f1s),\n",
    "        \"exact_match\": sum(ems)/len(ems),\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bert_score_f1\": bert_f1,\n",
    "        \"bert_score_R\": bert_R,\n",
    "        \"list_bert\": F1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a8c48-c9ed-4f98-be5a-34e2b3e73913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"clinicalt57e57no_copydescription16batchfinaltokenized\"\n",
    "f1_total=[]\n",
    "for tipo in questions:\n",
    "    print(tipo.upper())\n",
    "    with open(f\"Generativos/truth_dic/{tipo}.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "    with open(f\"Generativos/results_dic_{tipo}/{model}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "    truth = {\n",
    "    k: (\"no response.\" if v in list(not_present_dic.values()) else v)\n",
    "    for k, v in truth.items()\n",
    "    }\n",
    "    if tipo!=\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,tipo,point=True))\n",
    "    elif tipo==\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,tipo,point=\"mm\"))\n",
    "    else:\n",
    "        print(calculate_acc(output,truth,tipo))\n",
    "\n",
    "    result=calculate_metrics(output, truth, tipo)\n",
    "    f1_bio=result[\"list_bert\"]\n",
    "    f1_total+=f1_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee18fd-9a60-4222-9acf-be926a0cc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"biogpt\"+\"1e57\"+\"no_copy\"+\"description\"+\"16batch\"+\"final\"\n",
    "f1_total=[]\n",
    "for tipo in questions:\n",
    "    print(tipo.upper())\n",
    "    with open(f\"Generativos/truth_dic/{tipo}.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "    with open(f\"Generativos/results_dic_{tipo}/{model}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "    truth = {\n",
    "    k: (\"no response.\" if v in list(not_present_dic.values()) else v)\n",
    "    for k, v in truth.items()\n",
    "    }\n",
    "    if tipo!=\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,tipo,point=True))\n",
    "    elif tipo==\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,tipo,point=\"mm\"))\n",
    "    else:\n",
    "        print(calculate_acc(output,truth,tipo))\n",
    "\n",
    "    result=calculate_metrics(output, truth, tipo)\n",
    "    f1_bio=result[\"list_bert\"]\n",
    "    f1_total+=f1_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc4688-1826-4654-9025-7a0b30ce78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"biogpt\"+\"21e51e57\"+\"no_copy\"+\"description\"+\"16batch\"+\"final\"\n",
    "for tipo in questions:\n",
    "    print(tipo.upper())\n",
    "    with open(f\"Generativos/truth_dic/{tipo}.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "    with open(f\"Generativos/results_dic_{tipo}/{model_name}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "    if tipo!=\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,point=True))\n",
    "    elif tipo==\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,point=\"mm\"))\n",
    "    else:\n",
    "        print(calculate_acc(output,truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e769e-3fdd-4aa2-bd5d-59e6fd3b7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"biogpt\"+\"5e65e63\"\n",
    "for tipo in questions:\n",
    "    print(tipo.upper())\n",
    "    with open(f\"Generativos/truth_dic/{tipo}.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "    with open(f\"Generativos/results_dic_{tipo}/{model}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "    if tipo==\"history\" or tipo==\"family\" or tipo==\"symtomatic\" or tipo==\"density_echo\" or tipo==\"nodules_echo_echogenicity\" or tipo==\"nodules_echo_margin\" or tipo==\"nodules_echo_shape\"  or tipo==\"nodules_echo_location\":\n",
    "        print(calculate_acc(output,truth,point=True))\n",
    "    elif tipo==\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,point=\"mm\"))\n",
    "    else:\n",
    "        print(calculate_acc(output,truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82664845-c050-49dd-9521-b7e95bf23d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"biogpt1e55e63\"\n",
    "for tipo in questions:\n",
    "    print(tipo.upper())\n",
    "    with open(f\"Generativos/truth_dic/{tipo}.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "    with open(f\"Generativos/results_dic_{tipo}/{model}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "    if tipo==\"history\" or tipo==\"family\" or tipo==\"symtomatic\" or tipo==\"density_echo\" or tipo==\"nodules_echo_echogenicity\" or tipo==\"nodules_echo_margin\" or tipo==\"nodules_echo_shape\"  or tipo==\"nodules_echo_location\":\n",
    "        print(calculate_acc(output,truth,point=True))\n",
    "    elif tipo==\"nodules_echo_size\":\n",
    "        print(calculate_acc(output,truth,point=\"mm\"))\n",
    "    else:\n",
    "        print(calculate_acc(output,truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589797c9-f8fa-49a1-86c3-f0b9a1f6e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_acc(biogpt_age,truth_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed09c6-b224-44c8-9d1d-38e0b5ec8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_acc(biogpt_tipo,truth_tipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5378885-cfa8-43e4-b485-c8d255fc1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_acc(biogpt_tecnica,truth_tecnica)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
