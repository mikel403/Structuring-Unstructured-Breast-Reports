{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c5a20a-1b7f-411c-a306-20e170d1c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from bert_score import score as bert_score_fn\n",
    "\n",
    "import re\n",
    "import string\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b39476-6cab-4a46-9817-c9268bddab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def calculate_f1(pred,truth,point=False,average='macro'):\n",
    "    pred_clean={}\n",
    "    truth_clean={}\n",
    "    for key,real in truth.items():\n",
    "        if key in pred:\n",
    "            \n",
    "            predicted=pred[key] \n",
    "            if point==True:\n",
    "                predicted=predicted.split(\".\")[0]+\".\"\n",
    "            elif point==\"mm\":\n",
    "                predicted=predicted.split(\"mm\")[0]+\"mm.\"\n",
    "                \n",
    "            pred_clean[key]=predicted.lower()\n",
    "            truth_clean[key]=str(real).lower()\n",
    "    f1=f1_score(list(truth_clean.values()),list(pred_clean.values()),average=average)\n",
    "    return f1\n",
    "def calculate_acc(pred,truth,point=False):\n",
    "    acc=0\n",
    "    total=0\n",
    "    for key,real in truth.items():\n",
    "        if key in pred:\n",
    "            total+=1\n",
    "            predicted=pred[key]    \n",
    "            if point==True:\n",
    "\n",
    "                predicted=predicted.split(\".\")[0]+\".\"\n",
    "\n",
    "            elif point==\"mm\":\n",
    "                predicted=predicted.split(\"mm\")[0]+\"mm.\"\n",
    "            if predicted.lower()==str(real).lower():\n",
    "                acc+=1\n",
    "            else:\n",
    "                print(key,\"Real: \",real,\"Predicted: \",predicted)\n",
    "        # else:\n",
    "        #     print(key)\n",
    "    print(acc)\n",
    "    print(total)\n",
    "    return acc/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881e97a-74aa-45aa-b6de-503b0acb26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch\t\n",
    "dmis-lab/biobert-base-cased-v1.15e-05616batch\n",
    "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78ab7bab-789a-4bba-bd7c-38195b1abe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def calculate_metrics(pred, truth, tipo):\n",
    "    preds, refs = [], []\n",
    "    for key, real in truth.items():\n",
    "        if key not in pred:\n",
    "            continue\n",
    "        predicted = pred[key]    \n",
    "            \n",
    "        preds.append(predicted+\".\")\n",
    "        refs.append(real+\".\")\n",
    "\n",
    "    # Compute metrics\n",
    "    f1s, ems = [], []\n",
    "    for p, r in zip(preds, refs):\n",
    "        f1s.append(f1_score(p, r))\n",
    "        ems.append(int(normalize_answer(p) == normalize_answer(r)))\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    # BERTScore\n",
    "\n",
    "    P, R, F1 = bert_score_fn(preds, refs, lang=\"en\", verbose=False, rescale_with_baseline=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bert_R = R.mean().item()\n",
    "\n",
    "    print(f\"Examples: {len(preds)}\")\n",
    "    print(f\"Token F1: {sum(f1s)/len(f1s):.4f}\")\n",
    "    print(f\"Exact Match: {sum(ems)/len(ems):.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_f1:.4f}\")\n",
    "    print(f\"BERTScore R: {bert_R:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"token_f1\": sum(f1s)/len(f1s),\n",
    "        \"exact_match\": sum(ems)/len(ems),\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bert_score_f1\": bert_f1,\n",
    "        \"bert_score_R\": bert_R,\n",
    "        \"list_bert\": F1,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285bface-4f74-45aa-9475-870937957fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "173\n",
      "186\n",
      "parenchymal_distortion\n",
      "170\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "171\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "75\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "50\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "178\n",
      "186\n",
      "parenchymal_distortion\n",
      "175\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "174\n",
      "186\n",
      "parenchymal_distortion\n",
      "172\n",
      "186\n",
      "nodules_echo_size\n",
      "75\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "166\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "75\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "173\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "74\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "175\n",
      "186\n",
      "parenchymal_distortion\n",
      "173\n",
      "186\n",
      "nodules_echo_size\n",
      "76\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "168\n",
      "186\n",
      "parenchymal_distortion\n",
      "167\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "71\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "53\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "177\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "176\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "75\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "172\n",
      "186\n",
      "parenchymal_distortion\n",
      "171\n",
      "186\n",
      "nodules_echo_size\n",
      "77\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "74\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "64\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "173\n",
      "186\n",
      "parenchymal_distortion\n",
      "171\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "175\n",
      "186\n",
      "parenchymal_distortion\n",
      "172\n",
      "186\n",
      "nodules_echo_size\n",
      "76\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "169\n",
      "186\n",
      "parenchymal_distortion\n",
      "171\n",
      "186\n",
      "nodules_echo_size\n",
      "76\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "76\n",
      "82\n",
      "age\n",
      "172\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "67\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "175\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "176\n",
      "186\n",
      "parenchymal_distortion\n",
      "172\n",
      "186\n",
      "nodules_echo_size\n",
      "74\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "174\n",
      "186\n",
      "parenchymal_distortion\n",
      "170\n",
      "186\n",
      "nodules_echo_size\n",
      "76\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "167\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "201\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "171\n",
      "186\n",
      "parenchymal_distortion\n",
      "168\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "168\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "207\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "70\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "57\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "205\n",
      "212\n",
      "history\n",
      "160\n",
      "186\n",
      "parenchymal_distortion\n",
      "156\n",
      "186\n",
      "nodules_echo_size\n",
      "43\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "171\n",
      "186\n",
      "parenchymal_distortion\n",
      "167\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "66\n",
      "82\n",
      "age\n",
      "172\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "50\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "171\n",
      "186\n",
      "parenchymal_distortion\n",
      "169\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "172\n",
      "186\n",
      "parenchymal_distortion\n",
      "171\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "207\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "74\n",
      "82\n",
      "age\n",
      "166\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "48\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "174\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "70\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "174\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "166\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "74\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "67\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "47\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "172\n",
      "186\n",
      "parenchymal_distortion\n",
      "174\n",
      "186\n",
      "nodules_echo_size\n",
      "70\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "173\n",
      "186\n",
      "parenchymal_distortion\n",
      "173\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "170\n",
      "186\n",
      "parenchymal_distortion\n",
      "169\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "204\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "69\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "51\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "174\n",
      "186\n",
      "parenchymal_distortion\n",
      "173\n",
      "186\n",
      "nodules_echo_size\n",
      "71\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "172\n",
      "186\n",
      "parenchymal_distortion\n",
      "171\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "173\n",
      "186\n",
      "parenchymal_distortion\n",
      "169\n",
      "186\n",
      "nodules_echo_size\n",
      "73\n",
      "82\n",
      "age\n",
      "207\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "72\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "64\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "162\n",
      "186\n",
      "parenchymal_distortion\n",
      "165\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "201\n",
      "212\n",
      "history\n",
      "158\n",
      "186\n",
      "parenchymal_distortion\n",
      "162\n",
      "186\n",
      "nodules_echo_size\n",
      "41\n",
      "82\n",
      "age\n",
      "163\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "42\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "169\n",
      "186\n",
      "parenchymal_distortion\n",
      "165\n",
      "186\n",
      "nodules_echo_size\n",
      "41\n",
      "82\n",
      "age\n",
      "207\n",
      "212\n",
      "history\n",
      "157\n",
      "186\n",
      "parenchymal_distortion\n",
      "164\n",
      "186\n",
      "nodules_echo_size\n",
      "44\n",
      "82\n",
      "age\n",
      "197\n",
      "212\n",
      "history\n",
      "161\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "160\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "38\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "208\n",
      "212\n",
      "history\n",
      "167\n",
      "186\n",
      "parenchymal_distortion\n",
      "164\n",
      "186\n",
      "nodules_echo_size\n",
      "40\n",
      "82\n",
      "age\n",
      "207\n",
      "212\n",
      "history\n",
      "151\n",
      "186\n",
      "parenchymal_distortion\n",
      "155\n",
      "186\n",
      "nodules_echo_size\n",
      "48\n",
      "82\n",
      "age\n",
      "187\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "48\n",
      "82\n",
      "age\n",
      "168\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "42\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "3\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "167\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "40\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "166\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "41\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "162\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "44\n",
      "82\n",
      "age\n",
      "172\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "42\n",
      "82\n",
      "age\n",
      "156\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "7\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "172\n",
      "186\n",
      "parenchymal_distortion\n",
      "175\n",
      "186\n",
      "nodules_echo_size\n",
      "71\n",
      "82\n",
      "age\n",
      "211\n",
      "212\n",
      "history\n",
      "168\n",
      "186\n",
      "parenchymal_distortion\n",
      "163\n",
      "186\n",
      "nodules_echo_size\n",
      "49\n",
      "82\n",
      "age\n",
      "208\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "44\n",
      "82\n",
      "age\n",
      "195\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "159\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "37\n",
      "82\n",
      "age\n",
      "209\n",
      "212\n",
      "history\n",
      "166\n",
      "186\n",
      "parenchymal_distortion\n",
      "162\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "208\n",
      "212\n",
      "history\n",
      "169\n",
      "186\n",
      "parenchymal_distortion\n",
      "163\n",
      "186\n",
      "nodules_echo_size\n",
      "46\n",
      "82\n",
      "age\n",
      "210\n",
      "212\n",
      "history\n",
      "162\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "192\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "45\n",
      "82\n",
      "age\n",
      "166\n",
      "212\n",
      "history\n",
      "163\n",
      "186\n",
      "parenchymal_distortion\n",
      "166\n",
      "186\n",
      "nodules_echo_size\n",
      "39\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "accuracies=defaultdict(list)\n",
    "f1s=defaultdict(list)\n",
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "average=\"macro\"\n",
    "epochs=[3,4,5,6,7,8]\n",
    "lrs=[1e-4,5e-5,2e-5,1e-5,5e-6]\n",
    "models=[\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\"dmis-lab/biobert-base-cased-v1.1\",\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"]\n",
    "        \n",
    "for model_name in models:\n",
    "    for epoch in epochs:\n",
    "        for lr in lrs:\n",
    "            \n",
    "            model_name_str=model_name+str(lr)+str(epoch)+\"16batch\"\n",
    "            accuracies[\"model\"].append(model_name_str)\n",
    "            f1s[\"model\"].append(model_name_str)\n",
    "            for tipo in questions:\n",
    "                print(tipo)\n",
    "                with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "                        truth=pickle.load(file)\n",
    "                with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "                        output=pickle.load(file)\n",
    "                acc=calculate_acc(output,truth)\n",
    "                f1=calculate_f1(output,truth,average=average)\n",
    "                accuracies[tipo].append(acc)\n",
    "                f1s[tipo].append(f1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597d53e2-7efe-45b6-baa0-bc78b579f8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>history</th>\n",
       "      <th>parenchymal_distortion</th>\n",
       "      <th>nodules_echo_size</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001316batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.932403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05316batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.930436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05316batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.901391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.778606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001416batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.940860</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.942795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05416batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.942533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05416batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.923716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05416batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.836532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06416batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001516batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.939649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05516batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.940860</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.947091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05516batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.921651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05516batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.907488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06516batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.787752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.930961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.947909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.944598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.916635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06616batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001716batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.929519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.940860</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.946926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.937517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.922732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06716batch</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.849303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.940860</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.937419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.942172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.942894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.918962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06816batch</th>\n",
       "      <td>0.948113</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.889598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001316batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.923978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05316batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.920306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05316batch</th>\n",
       "      <td>0.976415</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.899723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.799947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001416batch</th>\n",
       "      <td>0.966981</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.797574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05416batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.922634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05416batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.892245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05416batch</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.797474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06416batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001516batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.928371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05516batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.932403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05516batch</th>\n",
       "      <td>0.976415</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.911918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05516batch</th>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.848325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06516batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.772508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.929977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.936075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05616batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.920667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05616batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.894114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06616batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.573171</td>\n",
       "      <td>0.769459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.927289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.936435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05716batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.925848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05716batch</th>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.893136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06716batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.781654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.931682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.932403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05816batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.931059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05816batch</th>\n",
       "      <td>0.976415</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.905820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06816batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001316batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.824353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05316batch</th>\n",
       "      <td>0.948113</td>\n",
       "      <td>0.849462</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.792136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05316batch</th>\n",
       "      <td>0.768868</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.762470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06316batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001416batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.821566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05416batch</th>\n",
       "      <td>0.976415</td>\n",
       "      <td>0.844086</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.809702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05416batch</th>\n",
       "      <td>0.929245</td>\n",
       "      <td>0.865591</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.809023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05416batch</th>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.746737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06416batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001516batch</th>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.812127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05516batch</th>\n",
       "      <td>0.976415</td>\n",
       "      <td>0.811828</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.801736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05516batch</th>\n",
       "      <td>0.882075</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.809065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05516batch</th>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.768366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06516batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.635313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001616batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.817173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05616batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.818878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05616batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.822648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05616batch</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.773083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06616batch</th>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.647508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.940860</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.931682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05716batch</th>\n",
       "      <td>0.995283</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.843103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05716batch</th>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.821634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05716batch</th>\n",
       "      <td>0.919811</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.809352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06716batch</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.742509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001816batch</th>\n",
       "      <td>0.985849</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.824518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05816batch</th>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.831763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05816batch</th>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.825697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05816batch</th>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.805815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06816batch</th>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.876344</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.475610</td>\n",
       "      <td>0.756861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         age   history  \\\n",
       "model                                                                    \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.930108   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.919355   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.956989   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990566  0.930108   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990566  0.940860   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.903226   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.951613   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.946237   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.924731   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.735849  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990566  0.930108   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.940860   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.908602   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.811321  0.876344   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.940860   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.946237   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.995283  0.897849   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.948113  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch      0.995283  0.919355   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch       0.995283  0.903226   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch       0.976415  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch      0.966981  0.860215   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch       0.995283  0.919355   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch       0.995283  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch       0.811321  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch      0.995283  0.919355   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch       0.995283  0.924731   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch       0.976415  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch       0.783019  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch      0.995283  0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch       0.995283  0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch       0.995283  0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch       0.990566  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch      0.995283  0.924731   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch       0.995283  0.930108   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch       0.990566  0.913978   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch       0.962264  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch       0.735849  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch      0.995283  0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch       0.995283  0.924731   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch       0.995283  0.930108   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch       0.976415  0.876344   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch       0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.870968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.948113  0.849462   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.768868  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.908602   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.976415  0.844086   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.929245  0.865591   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.754717  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.981132  0.897849   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.976415  0.811828   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.882075  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.792453  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.897849   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.870968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.811321  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.735849  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.995283  0.924731   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.995283  0.903226   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.981132  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.919811  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.750000  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.985849  0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.981132  0.908602   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990566  0.870968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.905660  0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.783019  0.876344   \n",
       "\n",
       "                                                    parenchymal_distortion  \\\n",
       "model                                                                        \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.913978   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.940860   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.924731   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.930108   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.897849   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.919355   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.919355   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.924731   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.919355   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.935484   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.924731   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.913978   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch                    0.903226   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch                    0.838710   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch                     0.897849   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch                    0.908602   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch                     0.919355   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch                    0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch                     0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch                    0.935484   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch                     0.930108   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch                     0.908602   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch                    0.930108   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch                     0.919355   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch                     0.908602   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch                     0.892473   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch                     0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.887097   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.870968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.887097   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.881720   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.881720   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.833333   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.940860   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.870968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.876344   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.892473   \n",
       "\n",
       "                                                    nodules_echo_size  \\\n",
       "model                                                                   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.890244   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.914634   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.841463   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.609756   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.036585   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.878049   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.914634   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.914634   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.841463   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.036585   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.902439   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926829   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.890244   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.865854   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.646341   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.841463   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.914634   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.939024   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.902439   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.780488   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.878049   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926829   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926829   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926829   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.817073   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.878049   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.902439   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926829   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.890244   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.841463   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch               0.878049   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch                0.853659   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch                0.695122   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch                0.036585   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch               0.524390   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch                0.878049   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch                0.804878   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch                0.609756   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch                0.036585   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch               0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch                0.902439   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch                0.841463   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch                0.585366   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch               0.853659   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch                0.878049   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch                0.902439   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch                0.817073   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch                0.573171   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch               0.853659   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch                0.841463   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch                0.621951   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch               0.865854   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch                0.890244   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch                0.878049   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch                0.780488   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.500000   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.512195   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.036585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.036585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.500000   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.536585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.463415   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.036585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.487805   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.585366   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.585366   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.512195   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.036585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.487805   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.500000   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.536585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.512195   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.085366   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.865854   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.597561   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.536585   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.451220   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.560976   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.548780   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.475610   \n",
       "\n",
       "                                                     average  \n",
       "model                                                         \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.932403  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.930436  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.901391  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.778606  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.635313  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.942795  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.942533  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.923716  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.836532  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.635313  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.939649  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.947091  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.921651  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.907488  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.787752  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.930961  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.947909  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.944598  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.916635  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.821289  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.929519  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.946926  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.937517  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.922732  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.849303  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.937419  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.942172  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.942894  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.918962  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.889598  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch      0.923978  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch       0.920306  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch       0.899723  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch       0.799947  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch       0.635313  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch      0.797574  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch       0.922634  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch       0.892245  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch       0.797474  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch       0.635313  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch      0.928371  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch       0.932403  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch       0.911918  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch       0.848325  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch       0.772508  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch      0.929977  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch       0.936075  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch       0.920667  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch       0.894114  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch       0.769459  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch      0.927289  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch       0.936435  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch       0.925848  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch       0.893136  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch       0.781654  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch      0.931682  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch       0.932403  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch       0.931059  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch       0.905820  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch       0.821289  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.824353  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.792136  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.762470  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.635313  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.635313  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.821566  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.809702  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.809023  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.746737  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.635313  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.812127  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.801736  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.809065  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.768366  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.635313  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.817173  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.818878  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.822648  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.773083  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.647508  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.931682  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.843103  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.821634  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.809352  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.742509  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.824518  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.831763  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.825697  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.805815  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.756861  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(accuracies)\n",
    "data=data.set_index(\"model\")\n",
    "data['average'] = data.mean(axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f448dc-c949-4d32-9246-9e711cbfc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch\t\n",
    "dmis-lab/biobert-base-cased-v1.15e-05616batch\n",
    "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b4283e-d331-4479-9488-df3ba094ae50",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1324788414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dmis-lab/biobert-base-cased-v1.10.0001416batch\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "dmis-lab/biobert-base-cased-v1.10.0001416batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3194a43-c7f7-498c-be9b-db3b4097a576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc1a3-8691-4add-a542-50990aa56af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch\t0.995283\t0.951613\t0.930108\t0.926829\t0.950958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa2b9f2-639e-49e7-820c-64c0c1f67a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>history</th>\n",
       "      <th>parenchymal_distortion</th>\n",
       "      <th>nodules_echo_size</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001316batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.433239</td>\n",
       "      <td>0.231310</td>\n",
       "      <td>0.858403</td>\n",
       "      <td>0.628334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05316batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.331735</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>0.919633</td>\n",
       "      <td>0.578474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05316batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.805861</td>\n",
       "      <td>0.472086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.432706</td>\n",
       "      <td>0.137257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001416batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.559396</td>\n",
       "      <td>0.347054</td>\n",
       "      <td>0.855095</td>\n",
       "      <td>0.687982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05416batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.383694</td>\n",
       "      <td>0.260740</td>\n",
       "      <td>0.889645</td>\n",
       "      <td>0.631116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05416batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.164039</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.526823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05416batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.785570</td>\n",
       "      <td>0.225473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06416batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001516batch</th>\n",
       "      <td>0.962785</td>\n",
       "      <td>0.383694</td>\n",
       "      <td>0.307005</td>\n",
       "      <td>0.875768</td>\n",
       "      <td>0.632313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05516batch</th>\n",
       "      <td>0.962785</td>\n",
       "      <td>0.439042</td>\n",
       "      <td>0.286493</td>\n",
       "      <td>0.921936</td>\n",
       "      <td>0.652564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05516batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.224894</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>0.850844</td>\n",
       "      <td>0.534672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05516batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.815873</td>\n",
       "      <td>0.474590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06516batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.152890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.564176</td>\n",
       "      <td>0.305952</td>\n",
       "      <td>0.789734</td>\n",
       "      <td>0.662562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.499002</td>\n",
       "      <td>0.279290</td>\n",
       "      <td>0.878648</td>\n",
       "      <td>0.661831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.319993</td>\n",
       "      <td>0.207301</td>\n",
       "      <td>0.926620</td>\n",
       "      <td>0.611075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.495621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06616batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.669180</td>\n",
       "      <td>0.196376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001716batch</th>\n",
       "      <td>0.962785</td>\n",
       "      <td>0.415307</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>0.840703</td>\n",
       "      <td>0.606446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.247919</td>\n",
       "      <td>0.920999</td>\n",
       "      <td>0.645393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.220907</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>0.895545</td>\n",
       "      <td>0.580221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.895545</td>\n",
       "      <td>0.494508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06716batch</th>\n",
       "      <td>0.343556</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.734023</td>\n",
       "      <td>0.292420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.460612</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>0.842704</td>\n",
       "      <td>0.656650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.519036</td>\n",
       "      <td>0.235125</td>\n",
       "      <td>0.848828</td>\n",
       "      <td>0.648344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract2e-05816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.394283</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.895545</td>\n",
       "      <td>0.615952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract1e-05816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.164602</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>0.858692</td>\n",
       "      <td>0.515243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-06816batch</th>\n",
       "      <td>0.841884</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.753347</td>\n",
       "      <td>0.421833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001316batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.306437</td>\n",
       "      <td>0.119849</td>\n",
       "      <td>0.841456</td>\n",
       "      <td>0.564532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05316batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.234162</td>\n",
       "      <td>0.045170</td>\n",
       "      <td>0.847507</td>\n",
       "      <td>0.529306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05316batch</th>\n",
       "      <td>0.936692</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.454348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.531404</td>\n",
       "      <td>0.161932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001416batch</th>\n",
       "      <td>0.857846</td>\n",
       "      <td>0.164364</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.379904</td>\n",
       "      <td>0.358674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05416batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.318580</td>\n",
       "      <td>0.111409</td>\n",
       "      <td>0.852960</td>\n",
       "      <td>0.568334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05416batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.721463</td>\n",
       "      <td>0.450987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05416batch</th>\n",
       "      <td>0.320087</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.431541</td>\n",
       "      <td>0.210932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06416batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001516batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.295301</td>\n",
       "      <td>0.156871</td>\n",
       "      <td>0.846726</td>\n",
       "      <td>0.572321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05516batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.373521</td>\n",
       "      <td>0.230824</td>\n",
       "      <td>0.856576</td>\n",
       "      <td>0.612826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05516batch</th>\n",
       "      <td>0.932883</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.847253</td>\n",
       "      <td>0.468059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05516batch</th>\n",
       "      <td>0.217417</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.742721</td>\n",
       "      <td>0.263060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06516batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.380150</td>\n",
       "      <td>0.124118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.438925</td>\n",
       "      <td>0.319286</td>\n",
       "      <td>0.794322</td>\n",
       "      <td>0.635730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.369592</td>\n",
       "      <td>0.319403</td>\n",
       "      <td>0.871955</td>\n",
       "      <td>0.637834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05616batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.134173</td>\n",
       "      <td>0.045170</td>\n",
       "      <td>0.886521</td>\n",
       "      <td>0.514062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05616batch</th>\n",
       "      <td>0.980770</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.730987</td>\n",
       "      <td>0.450964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06616batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.121581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.344098</td>\n",
       "      <td>0.332304</td>\n",
       "      <td>0.820005</td>\n",
       "      <td>0.621698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>0.299201</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.644424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05716batch</th>\n",
       "      <td>0.961723</td>\n",
       "      <td>0.278466</td>\n",
       "      <td>0.193883</td>\n",
       "      <td>0.867158</td>\n",
       "      <td>0.575307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05716batch</th>\n",
       "      <td>0.873571</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.787741</td>\n",
       "      <td>0.438353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06716batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.426077</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.10.0001816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.406738</td>\n",
       "      <td>0.255723</td>\n",
       "      <td>0.809710</td>\n",
       "      <td>0.615639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-05816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.332338</td>\n",
       "      <td>0.240441</td>\n",
       "      <td>0.855357</td>\n",
       "      <td>0.604630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.12e-05816batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.433239</td>\n",
       "      <td>0.144913</td>\n",
       "      <td>0.865457</td>\n",
       "      <td>0.608499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.11e-05816batch</th>\n",
       "      <td>0.942134</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.817802</td>\n",
       "      <td>0.463009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmis-lab/biobert-base-cased-v1.15e-06816batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.666071</td>\n",
       "      <td>0.195599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001316batch</th>\n",
       "      <td>0.980770</td>\n",
       "      <td>0.080443</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.405556</td>\n",
       "      <td>0.378444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05316batch</th>\n",
       "      <td>0.852939</td>\n",
       "      <td>0.035434</td>\n",
       "      <td>0.044335</td>\n",
       "      <td>0.358140</td>\n",
       "      <td>0.322712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05316batch</th>\n",
       "      <td>0.164829</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.378542</td>\n",
       "      <td>0.158868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06316batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001416batch</th>\n",
       "      <td>0.962785</td>\n",
       "      <td>0.248167</td>\n",
       "      <td>0.044898</td>\n",
       "      <td>0.330316</td>\n",
       "      <td>0.396541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05416batch</th>\n",
       "      <td>0.888474</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>0.044754</td>\n",
       "      <td>0.360788</td>\n",
       "      <td>0.338712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05416batch</th>\n",
       "      <td>0.769846</td>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.394590</td>\n",
       "      <td>0.313186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05416batch</th>\n",
       "      <td>0.083673</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.304370</td>\n",
       "      <td>0.120036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06416batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001516batch</th>\n",
       "      <td>0.927581</td>\n",
       "      <td>0.190498</td>\n",
       "      <td>0.046857</td>\n",
       "      <td>0.349583</td>\n",
       "      <td>0.378630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05516batch</th>\n",
       "      <td>0.903086</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.412454</td>\n",
       "      <td>0.342759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05516batch</th>\n",
       "      <td>0.653323</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.465638</td>\n",
       "      <td>0.302765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05516batch</th>\n",
       "      <td>0.251231</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.370382</td>\n",
       "      <td>0.178428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06516batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.029375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001616batch</th>\n",
       "      <td>0.961723</td>\n",
       "      <td>0.177296</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.331292</td>\n",
       "      <td>0.378388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05616batch</th>\n",
       "      <td>0.987120</td>\n",
       "      <td>0.157026</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.345950</td>\n",
       "      <td>0.384934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05616batch</th>\n",
       "      <td>0.980770</td>\n",
       "      <td>0.038905</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.386558</td>\n",
       "      <td>0.363968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05616batch</th>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.373692</td>\n",
       "      <td>0.195993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06616batch</th>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.034583</td>\n",
       "      <td>0.037727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.369478</td>\n",
       "      <td>0.497821</td>\n",
       "      <td>0.836231</td>\n",
       "      <td>0.673479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05716batch</th>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.267519</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.447396</td>\n",
       "      <td>0.437445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05716batch</th>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.039145</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.391875</td>\n",
       "      <td>0.355789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05716batch</th>\n",
       "      <td>0.710905</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.376273</td>\n",
       "      <td>0.294819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06716batch</th>\n",
       "      <td>0.069321</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.275968</td>\n",
       "      <td>0.109347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001816batch</th>\n",
       "      <td>0.936676</td>\n",
       "      <td>0.240125</td>\n",
       "      <td>0.040596</td>\n",
       "      <td>0.404311</td>\n",
       "      <td>0.405427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-05816batch</th>\n",
       "      <td>0.925570</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>0.109047</td>\n",
       "      <td>0.412331</td>\n",
       "      <td>0.415043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-122e-05816batch</th>\n",
       "      <td>0.961723</td>\n",
       "      <td>0.037457</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.402652</td>\n",
       "      <td>0.362868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-121e-05816batch</th>\n",
       "      <td>0.694197</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.385453</td>\n",
       "      <td>0.292938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-125e-06816batch</th>\n",
       "      <td>0.217417</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.049641</td>\n",
       "      <td>0.330032</td>\n",
       "      <td>0.159887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         age   history  \\\n",
       "model                                                                    \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.433239   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.331735   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.559396   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.383694   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.164039   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.962785  0.383694   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.962785  0.439042   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.224894   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.564176   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.499002   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.319993   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.024224  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.962785  0.415307   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.422268   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.220907   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.343556  0.042459   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.460612   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.519036   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.394283   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.990385  0.164602   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.841884  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch      0.990385  0.306437   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch       0.990385  0.234162   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch       0.936692  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch      0.857846  0.164364   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch       0.990385  0.318580   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch       0.990385  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch       0.320087  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch      0.990385  0.295301   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch       0.990385  0.373521   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch       0.932883  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch       0.217417  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch      0.990385  0.438925   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch       0.990385  0.369592   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch       0.990385  0.134173   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch       0.980770  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch      0.990385  0.344098   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch       0.990385  0.398810   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch       0.961723  0.278466   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch       0.873571  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch       0.024224  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch      0.990385  0.406738   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch       0.990385  0.332338   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch       0.990385  0.433239   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch       0.942134  0.042459   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch       0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.980770  0.080443   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.852939  0.035434   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.164829  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.962785  0.248167   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.888474  0.060832   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.769846  0.038665   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.083673  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.927581  0.190498   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.903086  0.025192   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.653323  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.251231  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.961723  0.177296   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.987120  0.157026   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.980770  0.038905   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.318182  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.024224  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990385  0.369478   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.990385  0.267519   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.942495  0.039145   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.710905  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.069321  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.936676  0.240125   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.925570  0.213223   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.961723  0.037457   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.694197  0.042459   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.217417  0.042459   \n",
       "\n",
       "                                                    parenchymal_distortion  \\\n",
       "model                                                                        \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.231310   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.072143   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.347054   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.260740   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.307005   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.286493   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.072564   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.305952   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.279290   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.207301   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.206989   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.247919   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.214045   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.332900   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.235125   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.183594   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.047293   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...                0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch                    0.119849   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch                     0.045170   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch                    0.032581   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch                     0.111409   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch                    0.156871   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch                     0.230824   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch                    0.319286   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch                     0.319403   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch                     0.045170   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch                    0.332304   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch                     0.299201   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch                     0.193883   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch                    0.255723   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch                     0.240441   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch                     0.144913   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch                     0.049641   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch                     0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.047009   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.044335   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.044898   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.044754   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.046857   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.030303   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.043240   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.497821   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.044481   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.040596   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.109047   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...                0.049641   \n",
       "\n",
       "                                                    nodules_echo_size  \\\n",
       "model                                                                   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.858403   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.919633   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.805861   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.432706   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.001176   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.855095   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.889645   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.903226   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.785570   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.001176   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.875768   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.921936   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.850844   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.815873   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.495238   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.789734   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.878648   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.926620   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.900000   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.669180   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.840703   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.920999   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.895545   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.895545   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.734023   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.842704   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.848828   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.895545   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.858692   \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...           0.753347   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch               0.841456   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch                0.847507   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch                0.788600   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch                0.531404   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch                0.001176   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch               0.379904   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch                0.852960   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch                0.721463   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch                0.431541   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch                0.001176   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch               0.846726   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch                0.856576   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch                0.847253   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch                0.742721   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch                0.380150   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch               0.794322   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch                0.871955   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch                0.886521   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch                0.730987   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch                0.370000   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch               0.820005   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch                0.889299   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch                0.867158   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch                0.787741   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch                0.426077   \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch               0.809710   \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch                0.855357   \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch                0.865457   \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch                0.817802   \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch                0.666071   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.405556   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.358140   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.378542   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.001176   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.001176   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.330316   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.360788   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.394590   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.304370   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.001176   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.349583   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.412454   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.465638   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.370382   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.001176   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.331292   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.345950   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.386558   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.373692   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.034583   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.836231   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.447396   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.391875   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.376273   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.275968   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.404311   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.412331   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.402652   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.385453   \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...           0.330032   \n",
       "\n",
       "                                                     average  \n",
       "model                                                         \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.628334  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.578474  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.472086  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.137257  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.029375  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.687982  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.631116  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.526823  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.225473  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.029375  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.632313  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.652564  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.534672  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.474590  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.152890  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.662562  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.661831  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.611075  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.495621  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.196376  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.606446  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.645393  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.580221  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.494508  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.292420  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.656650  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.648344  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.615952  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.515243  \n",
       "microsoft/BiomedNLP-PubMedBERT-base-uncased-abs...  0.421833  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001316batch      0.564532  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05316batch       0.529306  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05316batch       0.454348  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05316batch       0.161932  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06316batch       0.029375  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001416batch      0.358674  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05416batch       0.568334  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05416batch       0.450987  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05416batch       0.210932  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06416batch       0.029375  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001516batch      0.572321  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05516batch       0.612826  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05516batch       0.468059  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05516batch       0.263060  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06516batch       0.124118  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001616batch      0.635730  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05616batch       0.637834  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05616batch       0.514062  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05616batch       0.450964  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06616batch       0.121581  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001716batch      0.621698  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05716batch       0.644424  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05716batch       0.575307  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05716batch       0.438353  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06716batch       0.135600  \n",
       "dmis-lab/biobert-base-cased-v1.10.0001816batch      0.615639  \n",
       "dmis-lab/biobert-base-cased-v1.15e-05816batch       0.604630  \n",
       "dmis-lab/biobert-base-cased-v1.12e-05816batch       0.608499  \n",
       "dmis-lab/biobert-base-cased-v1.11e-05816batch       0.463009  \n",
       "dmis-lab/biobert-base-cased-v1.15e-06816batch       0.195599  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.378444  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.322712  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.158868  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.029375  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.029375  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.396541  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.338712  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.313186  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.120036  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.029375  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.378630  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.342759  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.302765  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.178428  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.029375  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.378388  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.384934  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.363968  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.195993  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.037727  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.673479  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.437445  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.355789  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.294819  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.109347  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.405427  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.415043  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.362868  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.292938  \n",
       "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768...  0.159887  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(f1s)\n",
    "data=data.set_index(\"model\")\n",
    "data['average'] = data.mean(axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd9cc4c7-f2bd-4954-953d-7649bb3a8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def calculate_metrics(pred, truth, tipo):\n",
    "    preds, refs = [], []\n",
    "    for key, real in truth.items():\n",
    "        if key not in pred:\n",
    "            continue\n",
    "        predicted = pred[key]    \n",
    "            \n",
    "        preds.append(predicted+\".\")\n",
    "        refs.append(real+\".\")\n",
    "\n",
    "    # Compute metrics\n",
    "    f1s, ems = [], []\n",
    "    for p, r in zip(preds, refs):\n",
    "        f1s.append(f1_score(p, r))\n",
    "        ems.append(int(normalize_answer(p) == normalize_answer(r)))\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    # BERTScore\n",
    "\n",
    "    P, R, F1 = bert_score_fn(preds, refs, lang=\"en\", verbose=False, rescale_with_baseline=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bert_R = R.mean().item()\n",
    "\n",
    "    print(f\"Examples: {len(preds)}\")\n",
    "    print(f\"Token F1: {sum(f1s)/len(f1s):.4f}\")\n",
    "    print(f\"Exact Match: {sum(ems)/len(ems):.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_f1:.4f}\")\n",
    "    print(f\"BERTScore R: {bert_R:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"token_f1\": sum(f1s)/len(f1s),\n",
    "        \"exact_match\": sum(ems)/len(ems),\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bert_score_f1\": bert_f1,\n",
    "        \"bert_score_R\": bert_R,\n",
    "        \"list_bert\": F1,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b38e5fd-dc62-4ab0-bc36-d764aed0befd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch\n",
      "age\n",
      "181-536-521-20221217-101331_age Real:  46 Predicted:  no response\n",
      "211\n",
      "212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 212\n",
      "Token F1: 0.9953\n",
      "Exact Match: 0.9953\n",
      "ROUGE-L: 0.9953\n",
      "BERTScore F1: 0.9976\n",
      "BERTScore R: 0.9982\n",
      "history\n",
      "144-007-361-20230619-165847_history Real:  reduction mammoplasty Predicted:  no response\n",
      "154-571-251-20230417-111004_history Real:  history of bilateral ca, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "154-571-251-20230928-113422_history Real:  bilateral breast cancer, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "227-053-436-20230714-113137_history Real:  operated twice in 2022 for left intraductal papillomas Predicted:  no response\n",
      "342-835-546-20230522-124057.921_history Real:  history of previous image - guided biopsy in right breast performed in another center with a benign pathological anatomical result Predicted:  no response\n",
      "400-124-635-20221219-134655.203_history Real:  history of follicular lymphoma in oct. 2021 Predicted:  no response\n",
      "646-504-461-20240506-082527_history Real:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer Predicted:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer in february 2021\n",
      "678-939-808-20220404-130020.625_history Real:  history of surgery for excision of fibroadenoma in the left breast Predicted:  history of surgery for excision of fibroadenoma\n",
      "766-258-279-20230831-132728.863_history Real:  operated of left breast cancer Predicted:  no response\n",
      "899-927-826-20230425-154430_history Real:  pathological anatomical papilloma removed with vacuum - assisted biopsy Predicted:  no response\n",
      "176\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9508\n",
      "Exact Match: 0.9462\n",
      "ROUGE-L: 0.9558\n",
      "BERTScore F1: 0.9628\n",
      "BERTScore R: 0.9558\n",
      "parenchymal_distortion\n",
      "043-372-637-20220620-135102_parenchymal_distortion Real:  focal asymmetry in the upper outer quadrant of the left breast, which corresponds to an accumulation of fibroglandular tissue that remains stable with respect to previous controls since 2020 Predicted:  focal asymmetry in the upper outer quadrant of the left breast\n",
      "058-197-862-20230511-152451_parenchymal_distortion Real:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast, currently there is a slight associated parenchymal distortion Predicted:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast\n",
      "095-672-078-20231010-170614_parenchymal_distortion Real:  parenchymal distortion in upper interquadrant - upper inner quadrant left breast, with associated pleomorphic microcalcifications Predicted:  parenchymal distortion in upper interquadrant - upper inner quadrant left breast\n",
      "185-342-604-20240514-121216_parenchymal_distortion Real:  post - surgical distortion in the upper outer quadrant, stable Predicted:  post - treatment changes in the left breast\n",
      "236-259-310-20230522-091930_parenchymal_distortion Real:  asymmetric density in outer quadrants of the right breast, deep third without apparent ultrasound translation, stable Predicted:  no response\n",
      "300-089-605-20230622-174812_parenchymal_distortion Real:  nonspecific asymmetric density in lower quadrants of the right breast Predicted:  no response\n",
      "346-239-757-20230622-171503_parenchymal_distortion Real:  asymmetry in the upper quadrant of the right breast Predicted:  asymmetry in the upper quadrant of the right breast. benign calcifications in both breasts\n",
      "358-702-179-20221219-133215.062_parenchymal_distortion Real:  focal asymmetry in radial 12 hours of the right breast, stable with respect to previous studies Predicted:  no response\n",
      "361-920-351-20221219-123950_parenchymal_distortion Real:  asymmetry in the upper outer quadrant of the left breast, stable with respect to previous studies Predicted:  asymmetry in the upper outer quadrant of the left breast, stable\n",
      "365-671-706-20230612-125943.734_parenchymal_distortion Real:  asymmetric density in the upper outer quadrant of the right breast, which dissociates with tomosynthesis, apparently in relation to fibroglandular tissue Predicted:  no response\n",
      "534-723-539-20231013-165228.835_parenchymal_distortion Real:  post - surgical alterations in the upper outer periareolar quadrant of the right breast Predicted:  no response\n",
      "654-622-062-20230524-121341_parenchymal_distortion Real:  post - treatment changes in left breast Predicted:  no response\n",
      "174\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9533\n",
      "Exact Match: 0.9355\n",
      "ROUGE-L: 0.9588\n",
      "BERTScore F1: 0.9623\n",
      "BERTScore R: 0.9521\n",
      "nodules_echo_size\n",
      "008-536-472-20211122-103547_nodules_echo_size_1 Real:  23 mm Predicted:  no response\n",
      "145-861-121-20220621-123317.718_nodules_echo_size_1 Real:  no response Predicted:  6 - 7 mm\n",
      "236-259-310-20230522-091930_nodules_echo_size_1 Real:  subcentimeter Predicted:  no response\n",
      "446-217-126-20230612-114316.187_nodules_echo_size_1 Real:  7. 7 - 9. 7 mm Predicted:  no response\n",
      "534-723-539-20231013-165228.835_nodules_echo_size_1 Real:  8 mm Predicted:  13 mm\n",
      "646-504-461-20240506-082527_nodules_echo_size_1 Real:  no response Predicted:  7, 6 and 5 mm\n",
      "766-258-279-20230831-132728.863_nodules_echo_size_1 Real:  21x8mm Predicted:  26 mm\n",
      "75\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 82\n",
      "Token F1: 0.9187\n",
      "Exact Match: 0.9146\n",
      "ROUGE-L: 0.9207\n",
      "BERTScore F1: 0.9477\n",
      "BERTScore R: 0.9466\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.251, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1429, 0.1688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1404, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.124, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2197, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8106, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8983, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5846, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7448, 1.0, 1.0, 1.0, 1.0, 0.7993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5135, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1061, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0921, 1.0, 1.0, 0.791, 1.0, 0.1715, 0.8349, 0.0428, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0492, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4213, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.215, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2746, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1416, 1.0, 1.0, 1.0, 0.9432, 1.0, 1.0, 1.0, 1.0, 0.1577, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5547, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "dmis-lab/biobert-base-cased-v1.15e-05616batch\n",
      "age\n",
      "181-536-521-20221217-101331_age Real:  46 Predicted:  no response\n",
      "211\n",
      "212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 212\n",
      "Token F1: 0.9953\n",
      "Exact Match: 0.9953\n",
      "ROUGE-L: 0.9953\n",
      "BERTScore F1: 0.9976\n",
      "BERTScore R: 0.9982\n",
      "history\n",
      "008-536-472-20211122-103547_history Real:  history of left breast fibroadenoma excision that had grown in 2017 Predicted:  history\n",
      "144-007-361-20230619-165847_history Real:  reduction mammoplasty Predicted:  currently in treatment of a glioblastoma\n",
      "154-571-251-20230417-111004_history Real:  history of bilateral ca, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "154-571-251-20230928-113422_history Real:  bilateral breast cancer, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "183-814-707-20220425-122932_history Real:  history of bilateral breast reduction surgery Predicted:  no response\n",
      "227-053-436-20230714-113137_history Real:  operated twice in 2022 for left intraductal papillomas Predicted:  no response\n",
      "400-124-635-20221219-134655.203_history Real:  history of follicular lymphoma in oct. 2021 Predicted:  no response\n",
      "591-887-183-20230612-122010.531_history Real:  history of biopsy of microcalcifications in the left breast on two occasions Predicted:  history of biopsy of microcalcifications in the left breast\n",
      "646-504-461-20240506-082527_history Real:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer Predicted:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer in february 2021\n",
      "678-939-808-20220404-130020.625_history Real:  history of surgery for excision of fibroadenoma in the left breast Predicted:  history of surgery for excision of fibroadenoma in the left breast, according to the patient\n",
      "766-258-279-20230831-132728.863_history Real:  operated of left breast cancer Predicted:  no response\n",
      "899-927-826-20230425-154430_history Real:  pathological anatomical papilloma removed with vacuum - assisted biopsy Predicted:  no response\n",
      "174\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9455\n",
      "Exact Match: 0.9355\n",
      "ROUGE-L: 0.9502\n",
      "BERTScore F1: 0.9576\n",
      "BERTScore R: 0.9515\n",
      "parenchymal_distortion\n",
      "043-372-637-20220620-135102_parenchymal_distortion Real:  focal asymmetry in the upper outer quadrant of the left breast, which corresponds to an accumulation of fibroglandular tissue that remains stable with respect to previous controls since 2020 Predicted:  focal asymmetry in the upper outer quadrant of the left breast\n",
      "058-197-862-20230511-152451_parenchymal_distortion Real:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast, currently there is a slight associated parenchymal distortion Predicted:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast\n",
      "095-672-078-20231010-170614_parenchymal_distortion Real:  parenchymal distortion in upper interquadrant - upper inner quadrant left breast, with associated pleomorphic microcalcifications Predicted:  no response\n",
      "153-234-011-20230824-150145.697_parenchymal_distortion Real:  focal asymmetry with a tendency to nodularity and imprecise contours located in the upper quadrants of the right breast Predicted:  focal\n",
      "170-615-388-20240610-121721.660_parenchymal_distortion Real:  post - treatment changes in the left breast, with skin thickening and trabeculation of the breast tissue and post - surgical distortion in the upper outer quadrant, stable Predicted:  post - surgical distortion in the upper outer quadrant, stable\n",
      "185-342-604-20240514-121216_parenchymal_distortion Real:  post - surgical distortion in the upper outer quadrant, stable Predicted:  post - treatment changes in the left breast, with skin thickening and trabeculation of the breast tissue and post - surgical distortion in the upper outer quadrant, stable\n",
      "358-702-179-20221219-133215.062_parenchymal_distortion Real:  focal asymmetry in radial 12 hours of the right breast, stable with respect to previous studies Predicted:  focal asymmetry in radial 12 hours of the right breast\n",
      "361-920-351-20221219-123950_parenchymal_distortion Real:  asymmetry in the upper outer quadrant of the left breast, stable with respect to previous studies Predicted:  asymmetry in the upper outer quadrant of the left breast\n",
      "365-671-706-20230612-125943.734_parenchymal_distortion Real:  asymmetric density in the upper outer quadrant of the right breast, which dissociates with tomosynthesis, apparently in relation to fibroglandular tissue Predicted:  no response\n",
      "534-723-539-20231013-165228.835_parenchymal_distortion Real:  post - surgical alterations in the upper outer periareolar quadrant of the right breast Predicted:  no response\n",
      "654-622-062-20230524-121341_parenchymal_distortion Real:  post - treatment changes in left breast Predicted:  no response\n",
      "792-710-587-20230529-130211.203_parenchymal_distortion Real:  no response Predicted:  superior periareolar asymmetry in left breast, stable with respect to previous studies\n",
      "174\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9547\n",
      "Exact Match: 0.9355\n",
      "ROUGE-L: 0.9584\n",
      "BERTScore F1: 0.9598\n",
      "BERTScore R: 0.9500\n",
      "nodules_echo_size\n",
      "001-689-096-20230529-131355_nodules_echo_size_1 Real:  8 - 9 mm Predicted:  no response\n",
      "145-861-121-20220621-123317.718_nodules_echo_size_1 Real:  no response Predicted:  6 - 7 mm\n",
      "236-259-310-20230522-091930_nodules_echo_size_1 Real:  subcentimeter Predicted:  no response\n",
      "261-477-582-20230602-103817_nodules_echo_size_1 Real:  7 mm Predicted:  6x3 mm\n",
      "290-754-149-20230823-105957_nodules_echo_size_1 Real:  9x5mm Predicted:  no response\n",
      "446-217-126-20230612-114316.187_nodules_echo_size_1 Real:  7. 7 - 9. 7 mm Predicted:  no response\n",
      "534-723-539-20231013-165228.835_nodules_echo_size_1 Real:  8 mm Predicted:  no response\n",
      "646-504-461-20240506-082527_nodules_echo_size_1 Real:  no response Predicted:  7, 6 and 5 mm\n",
      "678-939-808-20220404-130020.625_nodules_echo_size_1 Real:  6 mm Predicted:  no response\n",
      "766-258-279-20230831-132728.863_nodules_echo_size_1 Real:  21x8mm Predicted:  no response\n",
      "72\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 82\n",
      "Token F1: 0.8821\n",
      "Exact Match: 0.8780\n",
      "ROUGE-L: 0.8841\n",
      "BERTScore F1: 0.9105\n",
      "BERTScore R: 0.9084\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1663, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1429, 0.1688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.249, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1404, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2197, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8914, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8106, 1.0, 1.0, 1.0, 1.0, 1.0, 0.856, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5846, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7448, 1.0, 1.0, 1.0, 1.0, -0.0499, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2108, 1.0, 1.0, 1.0, 1.0, 0.4753, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4753, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7898, 0.7826, 0.0428, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0492, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1646, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.215, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2746, 1.0, 0.548, 1.0, 1.0, 1.0, 0.1628, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1416, 1.0, 1.0, 1.0, 0.4277, 1.0, 1.0, 1.0, 1.0, 0.1577, 1.0, 1.0, 1.0, 0.3544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1581, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch\n",
      "age\n",
      "181-536-521-20221217-101331_age Real:  46 Predicted:  no response\n",
      "211\n",
      "212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 212\n",
      "Token F1: 0.9953\n",
      "Exact Match: 0.9953\n",
      "ROUGE-L: 0.9953\n",
      "BERTScore F1: 0.9976\n",
      "BERTScore R: 0.9982\n",
      "history\n",
      "008-536-472-20211122-103547_history Real:  history of left breast fibroadenoma excision that had grown in 2017 Predicted:  history of left breast fibroadenoma excision that had grown in 2017. a comparative study is performed with respect to previous scans. comments : mammography : craniocaudal and oblique projection of both breasts with tomosynthesis and synthesized image ( c - view ). high density and heterogeneous breasts ( acr c ) which significantly reduces the sensitivity for the detection of underlying lesions. however, no obvious parenchymal distortions or suspicious calcifications are observed. benign calcifications in the right breast. circumscribed, oval and hypodense bilateral nodules, stable with respect to previous studies\n",
      "046-630-291-20230619-112356_history Real:  no response Predicted:  reduction mammoplasty\n",
      "144-007-361-20230619-165847_history Real:  reduction mammoplasty Predicted:  no response\n",
      "154-571-251-20230417-111004_history Real:  history of bilateral ca, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "154-571-251-20230928-113422_history Real:  bilateral breast cancer, right breast with la in 2001 and left breast in 2012 Predicted:  no response\n",
      "170-615-388-20240610-121721.660_history Real:  history of left breast cancer with conservative treatment Predicted:  history of left breast cancer\n",
      "185-342-604-20240514-121216_history Real:  history of left breast cancer with conservative treatment Predicted:  history of left breast cancer\n",
      "227-053-436-20230714-113137_history Real:  operated twice in 2022 for left intraductal papillomas Predicted:  no response\n",
      "400-124-635-20221219-134655.203_history Real:  history of follicular lymphoma in oct. 2021 Predicted:  no response\n",
      "591-887-183-20230612-122010.531_history Real:  history of biopsy of microcalcifications in the left breast on two occasions Predicted:  history of biopsy of microcalcifications in the left breast\n",
      "646-504-461-20240506-082527_history Real:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer Predicted:  bilateral mastectomy and reconstruction with prosthesis due to bilateral breast cancer in february 2021\n",
      "678-939-808-20220404-130020.625_history Real:  history of surgery for excision of fibroadenoma in the left breast Predicted:  history of surgery for excision of fibroadenoma in the left breast, according to the patient\n",
      "766-258-279-20230831-132728.863_history Real:  operated of left breast cancer Predicted:  no response\n",
      "899-927-826-20230425-154430_history Real:  pathological anatomical papilloma removed with vacuum - assisted biopsy Predicted:  no response\n",
      "172\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9435\n",
      "Exact Match: 0.9247\n",
      "ROUGE-L: 0.9480\n",
      "BERTScore F1: 0.9564\n",
      "BERTScore R: 0.9525\n",
      "parenchymal_distortion\n",
      "043-372-637-20220620-135102_parenchymal_distortion Real:  focal asymmetry in the upper outer quadrant of the left breast, which corresponds to an accumulation of fibroglandular tissue that remains stable with respect to previous controls since 2020 Predicted:  focal asymmetry in the upper outer quadrant of the left breast\n",
      "058-197-862-20230511-152451_parenchymal_distortion Real:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast, currently there is a slight associated parenchymal distortion Predicted:  focal asymmetry persists in the upper quadrant - upper outer quadrant unit of the left breast\n",
      "095-672-078-20231010-170614_parenchymal_distortion Real:  parenchymal distortion in upper interquadrant - upper inner quadrant left breast, with associated pleomorphic microcalcifications Predicted:  parenchymal distortion in upper interquadrant - upper inner quadrant left breast\n",
      "144-007-361-20230619-165847_parenchymal_distortion Real:  post - surgical changes without evidence of focal lesions of pathological significance Predicted:  no response\n",
      "154-571-251-20230417-111004_parenchymal_distortion Real:  no response Predicted:  post\n",
      "154-571-251-20230928-113422_parenchymal_distortion Real:  no response Predicted:  post\n",
      "185-342-604-20240514-121216_parenchymal_distortion Real:  post - surgical distortion in the upper outer quadrant, stable Predicted:  post - treatment changes in the left breast, with skin thickening and trabeculation of the breast tissue and post - surgical distortion in the upper outer quadrant, stable\n",
      "236-259-310-20230522-091930_parenchymal_distortion Real:  post - treatment changes in left breast. asymmetric density in outer quadrants of the right breast, deep third without apparent ultrasound translation, stable Predicted:  no response\n",
      "365-671-706-20230612-125943.734_parenchymal_distortion Real:  asymmetric density in the upper outer quadrant of the right breast, which dissociates with tomosynthesis, apparently in relation to fibroglandular tissue Predicted:  no response\n",
      "534-723-539-20231013-165228.835_parenchymal_distortion Real:  post - surgical alterations in the upper outer periareolar quadrant of the right breast Predicted:  no response\n",
      "766-258-279-20230831-132728.863_parenchymal_distortion Real:  post - surgical changes in left breast Predicted:  no response\n",
      "175\n",
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 186\n",
      "Token F1: 0.9518\n",
      "Exact Match: 0.9409\n",
      "ROUGE-L: 0.9559\n",
      "BERTScore F1: 0.9637\n",
      "BERTScore R: 0.9547\n",
      "nodules_echo_size\n",
      "095-672-078-20231010-170614_nodules_echo_size_1 Real:  7x10mm Predicted:  no response\n",
      "145-861-121-20220621-123317.718_nodules_echo_size_1 Real:  no response Predicted:  6 - 7 mm\n",
      "236-259-310-20230522-091930_nodules_echo_size_1 Real:  subcentimeter Predicted:  no response\n",
      "261-477-582-20230602-103817_nodules_echo_size_1 Real:  7 mm Predicted:  6x3 mm\n",
      "300-089-605-20230622-174812_nodules_echo_size_1 Real:  33 x. 18 mm Predicted:  33 x\n",
      "520-049-440-20230417-114625_nodules_echo_size_1 Real:  9. 7 mm Predicted:  no response\n",
      "534-723-539-20231013-165228.835_nodules_echo_size_1 Real:  8 mm Predicted:  13 mm\n",
      "646-504-461-20240506-082527_nodules_echo_size_1 Real:  no response Predicted:  7, 6 and 5 mm\n",
      "678-939-808-20220404-130020.625_nodules_echo_size_1 Real:  6 mm Predicted:  no response\n",
      "747-999-147-20230404-185900_nodules_echo_size_1 Real:  36mm Predicted:  no response\n",
      "949-173-639-20230123-110936_nodules_echo_size_1 Real:  12 mm Predicted:  10 mm\n",
      "71\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 82\n",
      "Token F1: 0.8854\n",
      "Exact Match: 0.8659\n",
      "ROUGE-L: 0.8923\n",
      "BERTScore F1: 0.9232\n",
      "BERTScore R: 0.9256\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2884, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.251, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.251, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1429, 0.1688, 1.0, 1.0, 0.7745, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7745, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1404, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2197, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8914, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8106, 1.0, 1.0, 1.0, 1.0, 1.0, 0.856, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5846, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7448, 1.0, 1.0, 1.0, 1.0, 0.7993, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1763, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5196, 0.5196, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4753, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0862, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0428, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0492, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.259, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1539, 1.0, 1.0, 0.215, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2746, 1.0, 0.548, 1.0, 1.0, 1.0, 1.0, 0.3508, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3658, 0.9432, 1.0, 1.0, 1.0, 1.0, 0.1577, 1.0, 1.0, 1.0, 0.3544, 1.0, 1.0, 1.0, 1.0, 0.372, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9675, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "accuracies=defaultdict(list)\n",
    "f1s=defaultdict(list)\n",
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "average=\"macro\"\n",
    "epochs=[3,4,5,6,7,8]\n",
    "lrs=[1e-4,5e-5,2e-5,1e-5,5e-6]\n",
    "models=[\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\"dmis-lab/biobert-base-cased-v1.1\",\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"]\n",
    "\n",
    "BioMedBERT=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch\"\n",
    "BlueBERT=\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch\"\n",
    "BioBERT=\"dmis-lab/biobert-base-cased-v1.10.0001416batch\"\n",
    "\n",
    "BioMedBERT=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch\"\n",
    "BioBERT=\"dmis-lab/biobert-base-cased-v1.15e-05616batch\"\n",
    "BlueBERT=\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch\"\n",
    "models=[BioMedBERT, BioBERT,BlueBERT]\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    f1_total=[]\n",
    "    model_name_str=model_name\n",
    "    accuracies[\"model\"].append(model_name_str)\n",
    "    f1s[\"model\"].append(model_name_str)\n",
    "    for tipo in questions:\n",
    "        print(tipo)\n",
    "        with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "        with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "        acc=calculate_acc(output,truth)\n",
    "        # f1=calculate_f1(output,truth,average=average)\n",
    "        result=calculate_metrics(output, truth, tipo)\n",
    "        f1_bio=result[\"list_bert\"]\n",
    "        \n",
    "        f1_total+=f1_bio\n",
    "        \n",
    "        accuracies[tipo].append(acc)\n",
    "    print([round(v.item(),4) for v in f1_total])\n",
    "        # f1s[tipo].append(f1)\n",
    "# for model_name in models:\n",
    "#     print(model_name)\n",
    "#     model_name_str=model_name\n",
    "#     accuracies[\"model\"].append(model_name_str)\n",
    "#     f1s[\"model\"].append(model_name_str)\n",
    "#     for tipo in questions:\n",
    "#         print(tipo)\n",
    "#         with open(f\"Generativos/results_dic_{tipo}/{\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch\".split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "#                 truth=pickle.load(file)\n",
    "#         with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "#                 output=pickle.load(file)\n",
    "#         acc=calculate_acc(output,truth)\n",
    "#         f1=calculate_f1(output,truth,average=average)\n",
    "#         accuracies[tipo].append(acc)\n",
    "#         f1s[tipo].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d93a8-938f-4ce7-9e4b-b250c0937869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
