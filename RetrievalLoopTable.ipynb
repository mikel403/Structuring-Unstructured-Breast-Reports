{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5a20a-1b7f-411c-a306-20e170d1c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from bert_score import score as bert_score_fn\n",
    "\n",
    "import re\n",
    "import string\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b39476-6cab-4a46-9817-c9268bddab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def calculate_f1(pred,truth,point=False,average='macro'):\n",
    "    pred_clean={}\n",
    "    truth_clean={}\n",
    "    for key,real in truth.items():\n",
    "        if key in pred:\n",
    "            \n",
    "            predicted=pred[key] \n",
    "            if point==True:\n",
    "                predicted=predicted.split(\".\")[0]+\".\"\n",
    "            elif point==\"mm\":\n",
    "                predicted=predicted.split(\"mm\")[0]+\"mm.\"\n",
    "                \n",
    "            pred_clean[key]=predicted.lower()\n",
    "            truth_clean[key]=str(real).lower()\n",
    "    f1=f1_score(list(truth_clean.values()),list(pred_clean.values()),average=average)\n",
    "    return f1\n",
    "def calculate_acc(pred,truth,point=False):\n",
    "    acc=0\n",
    "    total=0\n",
    "    for key,real in truth.items():\n",
    "        if key in pred:\n",
    "            total+=1\n",
    "            predicted=pred[key]    \n",
    "            if point==True:\n",
    "\n",
    "                predicted=predicted.split(\".\")[0]+\".\"\n",
    "\n",
    "            elif point==\"mm\":\n",
    "                predicted=predicted.split(\"mm\")[0]+\"mm.\"\n",
    "            if predicted.lower()==str(real).lower():\n",
    "                acc+=1\n",
    "            else:\n",
    "                print(key,\"Real: \",real,\"Predicted: \",predicted)\n",
    "        # else:\n",
    "        #     print(key)\n",
    "    print(acc)\n",
    "    print(total)\n",
    "    return acc/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab7bab-789a-4bba-bd7c-38195b1abe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def calculate_metrics(pred, truth, tipo):\n",
    "    preds, refs = [], []\n",
    "    for key, real in truth.items():\n",
    "        if key not in pred:\n",
    "            continue\n",
    "        predicted = pred[key]    \n",
    "            \n",
    "        preds.append(predicted+\".\")\n",
    "        refs.append(real+\".\")\n",
    "\n",
    "    # Compute metrics\n",
    "    f1s, ems = [], []\n",
    "    for p, r in zip(preds, refs):\n",
    "        f1s.append(f1_score(p, r))\n",
    "        ems.append(int(normalize_answer(p) == normalize_answer(r)))\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    # BERTScore\n",
    "\n",
    "    P, R, F1 = bert_score_fn(preds, refs, lang=\"en\", verbose=False, rescale_with_baseline=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bert_R = R.mean().item()\n",
    "\n",
    "    print(f\"Examples: {len(preds)}\")\n",
    "    print(f\"Token F1: {sum(f1s)/len(f1s):.4f}\")\n",
    "    print(f\"Exact Match: {sum(ems)/len(ems):.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_f1:.4f}\")\n",
    "    print(f\"BERTScore R: {bert_R:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"token_f1\": sum(f1s)/len(f1s),\n",
    "        \"exact_match\": sum(ems)/len(ems),\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bert_score_f1\": bert_f1,\n",
    "        \"bert_score_R\": bert_R,\n",
    "        \"list_bert\": F1,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bface-4f74-45aa-9475-870937957fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=defaultdict(list)\n",
    "f1s=defaultdict(list)\n",
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "average=\"macro\"\n",
    "epochs=[3,4,5,6,7,8]\n",
    "lrs=[1e-4,5e-5,2e-5,1e-5,5e-6]\n",
    "models=[\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\"dmis-lab/biobert-base-cased-v1.1\",\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"]\n",
    "        \n",
    "for model_name in models:\n",
    "    for epoch in epochs:\n",
    "        for lr in lrs:\n",
    "            \n",
    "            model_name_str=model_name+str(lr)+str(epoch)+\"16batch\"\n",
    "            accuracies[\"model\"].append(model_name_str)\n",
    "            f1s[\"model\"].append(model_name_str)\n",
    "            for tipo in questions:\n",
    "                print(tipo)\n",
    "                with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "                        truth=pickle.load(file)\n",
    "                with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "                        output=pickle.load(file)\n",
    "                acc=calculate_acc(output,truth)\n",
    "                f1=calculate_f1(output,truth,average=average)\n",
    "                accuracies[tipo].append(acc)\n",
    "                f1s[tipo].append(f1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d53e2-7efe-45b6-baa0-bc78b579f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(accuracies)\n",
    "data=data.set_index(\"model\")\n",
    "data['average'] = data.mean(axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2b9f2-639e-49e7-820c-64c0c1f67a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(f1s)\n",
    "data=data.set_index(\"model\")\n",
    "data['average'] = data.mean(axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cc4c7-f2bd-4954-953d-7649bb3a8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def calculate_metrics(pred, truth, tipo):\n",
    "    preds, refs = [], []\n",
    "    for key, real in truth.items():\n",
    "        if key not in pred:\n",
    "            continue\n",
    "        predicted = pred[key]    \n",
    "            \n",
    "        preds.append(predicted+\".\")\n",
    "        refs.append(real+\".\")\n",
    "\n",
    "    # Compute metrics\n",
    "    f1s, ems = [], []\n",
    "    for p, r in zip(preds, refs):\n",
    "        f1s.append(f1_score(p, r))\n",
    "        ems.append(int(normalize_answer(p) == normalize_answer(r)))\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    rouge_l = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    # BERTScore\n",
    "\n",
    "    P, R, F1 = bert_score_fn(preds, refs, lang=\"en\", verbose=False, rescale_with_baseline=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bert_R = R.mean().item()\n",
    "\n",
    "    print(f\"Examples: {len(preds)}\")\n",
    "    print(f\"Token F1: {sum(f1s)/len(f1s):.4f}\")\n",
    "    print(f\"Exact Match: {sum(ems)/len(ems):.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_f1:.4f}\")\n",
    "    print(f\"BERTScore R: {bert_R:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"token_f1\": sum(f1s)/len(f1s),\n",
    "        \"exact_match\": sum(ems)/len(ems),\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bert_score_f1\": bert_f1,\n",
    "        \"bert_score_R\": bert_R,\n",
    "        \"list_bert\": F1,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e5fd-dc62-4ab0-bc36-d764aed0befd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies=defaultdict(list)\n",
    "f1s=defaultdict(list)\n",
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "average=\"macro\"\n",
    "epochs=[3,4,5,6,7,8]\n",
    "lrs=[1e-4,5e-5,2e-5,1e-5,5e-6]\n",
    "models=[\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\"dmis-lab/biobert-base-cased-v1.1\",\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"]\n",
    "\n",
    "BioMedBERT=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch\"\n",
    "BlueBERT=\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch\"\n",
    "BioBERT=\"dmis-lab/biobert-base-cased-v1.10.0001416batch\"\n",
    "\n",
    "BioMedBERT=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract5e-05616batch\"\n",
    "BioBERT=\"dmis-lab/biobert-base-cased-v1.15e-05616batch\"\n",
    "BlueBERT=\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-120.0001716batch\"\n",
    "models=[BioMedBERT, BioBERT,BlueBERT]\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    f1_total=[]\n",
    "    model_name_str=model_name\n",
    "    accuracies[\"model\"].append(model_name_str)\n",
    "    f1s[\"model\"].append(model_name_str)\n",
    "    for tipo in questions:\n",
    "        print(tipo)\n",
    "        with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "            truth=pickle.load(file)\n",
    "        with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "            output=pickle.load(file)\n",
    "        acc=calculate_acc(output,truth)\n",
    "        # f1=calculate_f1(output,truth,average=average)\n",
    "        result=calculate_metrics(output, truth, tipo)\n",
    "        f1_bio=result[\"list_bert\"]\n",
    "        \n",
    "        f1_total+=f1_bio\n",
    "        \n",
    "        accuracies[tipo].append(acc)\n",
    "    print([round(v.item(),4) for v in f1_total])\n",
    "        # f1s[tipo].append(f1)\n",
    "# for model_name in models:\n",
    "#     print(model_name)\n",
    "#     model_name_str=model_name\n",
    "#     accuracies[\"model\"].append(model_name_str)\n",
    "#     f1s[\"model\"].append(model_name_str)\n",
    "#     for tipo in questions:\n",
    "#         print(tipo)\n",
    "#         with open(f\"Generativos/results_dic_{tipo}/{\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract0.0001816batch\".split(\"/\")[-1]}truth.pkl\", \"rb\") as file:\n",
    "#                 truth=pickle.load(file)\n",
    "#         with open(f\"Generativos/results_dic_{tipo}/{model_name_str.split(\"/\")[-1]}.pkl\", \"rb\") as file:\n",
    "#                 output=pickle.load(file)\n",
    "#         acc=calculate_acc(output,truth)\n",
    "#         f1=calculate_f1(output,truth,average=average)\n",
    "#         accuracies[tipo].append(acc)\n",
    "#         f1s[tipo].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d93a8-938f-4ce7-9e4b-b250c0937869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
