{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2282765-23ae-4a6b-9b13-38639b9bf8d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 154\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# if answer==word_to_idx_out[\"other\"]:\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;66;03m#     for j in range(2):\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m#         examples_raw[key+\"_copy\"+str(j)]=report\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# flattened_examples=pd.DataFrame.from_dict(flattened_examples,orient='index')\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# targets=pd.DataFrame.from_dict(targets,orient='index')\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flattened_examples,questions_examples,targets,examples_raw\n\u001b[0;32m--> 154\u001b[0m inputs,questions_examples,targets,examples_raw \u001b[38;5;241m=\u001b[39m flatten_and_filter_dataset(\u001b[43mground_truth\u001b[49m,report_data)  \n\u001b[1;32m    155\u001b[0m dataset_final\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(inputs,orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m questions_examples\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(questions_examples,orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "questions=[\"age\",\"history\",\"parenchymal_distortion\",\"nodules_echo_size\"]\n",
    "\n",
    "\n",
    "def fix_brackets_spaces(texto):\n",
    "    ''' \n",
    "        Introduce espacios por delante y por detrás de los paréntesis.\n",
    "        Esta medida mejora el tokenizado de Spacy\n",
    "    '''\n",
    "    \n",
    "    texto = re.sub(r'([(\\[¿!])', r' \\1', texto)\n",
    "    texto = re.sub(r'([)\\]?¡])', r'\\1 ', texto)\n",
    "                \n",
    "    return texto\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    text= fix_brackets_spaces(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def flatten_and_filter_dataset(ground_truth,reports):\n",
    "    \"\"\"\n",
    "    Esta función toma un conjunto de datos en el formato original (con estructura jerárquica)\n",
    "    y devuelve un conjunto de datos plano, donde cada entrada tiene un solo `context`, `question` y `answer`.\n",
    "    \n",
    "    Argumentos:\n",
    "        dataset: Un conjunto de datos en formato original (puede ser train, validation, test).\n",
    "    \n",
    "    Retorno:\n",
    "        Un conjunto de datos de Hugging Face en formato plano, con solo ejemplos completos.\n",
    "    \"\"\"\n",
    "    # Lista para almacenar ejemplos en formato plano\n",
    "    flattened_examples = {}\n",
    "    examples_raw={}\n",
    "    targets={}\n",
    "    questions_examples={}\n",
    "    val_data={}\n",
    "    question_tipo={}\n",
    "    previous_message_answer_tipo={}\n",
    "    options_tipo={}\n",
    "    answers_tipo={}\n",
    "    j=0\n",
    "    \n",
    "    question_tipo[\"age\"]= \"does the patient's age appear in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"age\"]=\"search for numbers, but do not mistake it with the age of a familiar. If a number appears without any context between two dots it is surely the age.\"\n",
    "    \n",
    "\n",
    "    question_tipo[\"history\"]= \"does the patient have any non-familiar history in the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"history\"]=\"check for the history at the beginning of the report. Normally it is a previous biopsy result, mastectomy or cancer. It normally starts with 'history of ...'.\"\n",
    "\n",
    "    question_tipo[\"parenchymal_distortion\"]= \"does the following breast medical report mention any parenchymal distortion or asymmetry in the mammography exam?\"\n",
    "    previous_message_answer_tipo[\"parenchymal_distortion\"]=\"If it has any it will appear in the results of the mammography exam using the words distortion, asymmetry or sometimes it can also be surgical changes.\"\n",
    "    \n",
    "\n",
    "    # question_tipo[\"nodules_echo_location_1\"]= \"In which location is the first nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    # previous_message_answer_tipo[\"nodules_echo_location_1\"]=\"do not consider if a nodule is described in the mammography exam or if it is in the axilla. If the nodule is mentioned previously in the mammography, the location can be found also there. sometimes the breast location of the tumour may be written in a different part than the quadrant. More than one nodule can be described simultaneously ('several', 'two', 'three', etc.). If the number is unspecified, only the ones with size will be considered. Sometimes it can also say 'similar characteristics' or 'similar to the previous', consider in this case the answer to the previous tumour.\"\n",
    "    \n",
    "    \n",
    "    question_tipo[\"nodules_echo_size_1\"]= \"what is the size of the first nodule described in the ultrasound exam of the following breast medical report?\"\n",
    "    previous_message_answer_tipo[\"nodules_echo_size_1\"]=\"do not consider if a nodule is described in the mammography exam. The localization, echogenicity and size of the nodules are normally said. More than one nodule can be described simultaneously ('several', 'two', 'three', etc.). If the number is unspecified, only the ones with size will be considered. Sometimes it can also say 'similar characteristics' or 'similar to the previous', consider in this case the answer to the previous tumour.\"\n",
    "    \n",
    "    # Iterar sobre cada ejemplo en el conjunto de datos original\n",
    "    for i, report in enumerate(reports[\"informes_ingles\"]):\n",
    "        informe=preprocess_text(report)\n",
    "        key=reports[\"keys\"][i]\n",
    "        \n",
    "        if key not in ground_truth.index:\n",
    "            continue\n",
    "        answer_tipo={}\n",
    "        row=ground_truth.loc[key]\n",
    "        age=str(row[\"Age\"])\n",
    "        # Verificar si el ejemplo tiene preguntas\n",
    "        if age.isdigit():\n",
    "            answer_tipo[\"age\"]=age\n",
    "            \n",
    "        else:\n",
    "            answer_tipo[\"age\"]=\"no response\"\n",
    "\n",
    "        if row[\"Biopsy_report\"]!=\"Yes\" and row[\"Ganglio_report\"]!=\"Yes\":\n",
    "            history=row[\"Other_history\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(history,str) or history==\"No\":\n",
    "                answer_tipo[\"history\"]=\"no response\"           \n",
    "            else:\n",
    "                answer_tipo[\"history\"]=history\n",
    "\n",
    "\n",
    "            #Parenchymal distortion\n",
    "            parenchymal_distortion=row[\"parenchymal_distortions_asymmetry\"]\n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(parenchymal_distortion,str) or parenchymal_distortion.lower()==\"no\":\n",
    "                answer_tipo[\"parenchymal_distortion\"]=\"no response\"        \n",
    "            else:\n",
    "                answer_tipo[\"parenchymal_distortion\"]=parenchymal_distortion.lower()\n",
    "\n",
    "            #Nodules echo\n",
    "            nodules_echo_num=row[\"Nodules_eco\"]\n",
    "            \n",
    "            # Verificar si el ejemplo tiene preguntas\n",
    "            if not isinstance(nodules_echo_num,str) and not isinstance(nodules_echo_num,int):\n",
    "                nodules=False\n",
    "            elif isinstance(nodules_echo_num,str) and nodules_echo_num==\"No\":\n",
    "                nodules=False\n",
    "            else:\n",
    "                nodules=True\n",
    "            if nodules:\n",
    "                # nodules_echo_location=row[\"Location_eco_1\"]\n",
    "                # # Verificar si el ejemplo tiene preguntas\n",
    "                # if not isinstance(nodules_echo_location,str):\n",
    "                #     answer_tipo[\"nodules_echo_location_1\"]=\"no response\"\n",
    "                # else:\n",
    "                #     answer_tipo[\"nodules_echo_location_1\"]=nodules_echo_location.lower()\n",
    "                #Nodules echo size\n",
    "                nodules_echo_size=row[\"size_eco_1\"]\n",
    "                # Verificar si el ejemplo tiene preguntas\n",
    "                if not isinstance(nodules_echo_size,str):\n",
    "                    answer_tipo[\"nodules_echo_size_1\"]=\"no response\"         \n",
    "                else:\n",
    "                    answer_tipo[\"nodules_echo_size_1\"]=nodules_echo_size\n",
    "                \n",
    "\n",
    "            \n",
    "        for tipo in answer_tipo:\n",
    "            #Si está el tipo en las respuestas que hemos recogido lo metemos a la base de datos.\n",
    "            key_tipo=key+\"_\"+tipo\n",
    "            if key_tipo in flattened_examples:\n",
    "                continue\n",
    "                key_tipo=key_tipo+\"_copy\"\n",
    "            examples_raw[key_tipo]=report\n",
    "            answer=str(answer_tipo[tipo])\n",
    "            \n",
    "            questions_examples[key_tipo]=question_tipo[tipo]\n",
    "            inputs_tipo = \" Additional information: \"+ previous_message_answer_tipo[tipo]+ \" Context: \" + informe\n",
    "            flattened_examples[key_tipo]=inputs_tipo\n",
    "            targets[key_tipo]=answer.strip()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if answer==word_to_idx_out[\"other\"]:\n",
    "        #     for j in range(2):\n",
    "        #         examples_raw[key+\"_copy\"+str(j)]=report\n",
    "        #         flattened_examples[key+\"_copy\"+str(j)]=informe\n",
    "        #         targets[key+\"_copy\"+str(j)]=answer\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "    # flattened_examples=pd.DataFrame.from_dict(flattened_examples,orient='index')\n",
    "    # targets=pd.DataFrame.from_dict(targets,orient='index')\n",
    "    return flattened_examples,questions_examples,targets,examples_raw\n",
    "\n",
    "\n",
    "inputs,questions_examples,targets,examples_raw = flatten_and_filter_dataset(ground_truth,report_data)  \n",
    "dataset_final=pd.DataFrame.from_dict(inputs,orient='index')\n",
    "questions_examples=pd.DataFrame.from_dict(questions_examples,orient='index')\n",
    "\n",
    "print(len(dataset_final),len(questions_examples),len(targets))\n",
    "\n",
    "targets=pd.DataFrame.from_dict(targets,orient='index')\n",
    "dataset_final.columns=[\"text\"]\n",
    "\n",
    "dataset_final[\"question\"]=questions_examples\n",
    "\n",
    "targets.columns=[\"label\"]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def visualize_errors(valid_dataset,valid_targets,validation_predictions,keys):\n",
    "    # Crear un DataFrame con los textos originales, las etiquetas reales y las predicciones\n",
    "    results_df = pd.DataFrame({\n",
    "        'key':list(keys),\n",
    "        'Text': list(valid_dataset),  # Usamos los textos originales\n",
    "        'True Label': list(valid_targets),\n",
    "        'Predicted Label': list(validation_predictions)\n",
    "    })\n",
    "    \n",
    "    # Filtrar los ejemplos en los que el modelo falló\n",
    "    errors_df = results_df[results_df['True Label'] != results_df['Predicted Label']]\n",
    "    \n",
    "    for ind,row in errors_df.iterrows():\n",
    "        print(row[\"key\"])\n",
    "        print(\"EJEMPLO\")\n",
    "        print(row[\"Text\"])\n",
    "        print(\"PREDICTED\")\n",
    "        print(row[\"Predicted Label\"])\n",
    "        print(\"TRUE\")\n",
    "        print(row[\"True Label\"])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    context_texts = examples[\"text\"]\n",
    "    answer_texts = examples[\"label\"]\n",
    "    errors=[]\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        context = context_texts[i]\n",
    "        answer = answer_texts[i]\n",
    "\n",
    "        # Default to CLS for no response\n",
    "        if answer == \"no response\" or answer.strip() == \"\":\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        # Lowercase match to avoid case mismatch\n",
    "        start_char = context.lower().find(answer.lower())\n",
    "        if start_char == -1:\n",
    "            print(i)\n",
    "            errors.append(i)\n",
    "            print(f\"[WARNING] Could not find answer: '{answer}' in context:\\n{context}\")\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "\n",
    "        end_char = start_char + len(answer)\n",
    "\n",
    "        # Now find token positions\n",
    "        start_pos = None\n",
    "        end_pos = None\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end and start_pos is None:\n",
    "                start_pos = idx\n",
    "            if start < end_char <= end:\n",
    "                end_pos = idx\n",
    "                break\n",
    "\n",
    "        if start_pos is None or end_pos is None:\n",
    "            # Fallback if something failed\n",
    "            print(f\"[WARNING] Failed to align answer '{answer}' in context\")\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_positions.append(start_pos)\n",
    "            end_positions.append(end_pos)\n",
    "        \n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples\n",
    "    \n",
    "def test_train_split(i, X, Y):\n",
    "    random.seed(1)\n",
    "    # Agrupar ejemplos originales y sus copias\n",
    "    def obtener_grupo(nombre):\n",
    "        # Extraer el nombre base eliminando '_copia' y cualquier número posterior\n",
    "        if '_copy' in nombre:\n",
    "            return nombre.split('_copy')[0]\n",
    "\n",
    "        return nombre\n",
    "\n",
    "    # Crear un DataFrame temporal para manejar los índices\n",
    "    agrupaciones = pd.DataFrame(index=X.index)\n",
    "    agrupaciones['grupo'] = agrupaciones.index.map(obtener_grupo)\n",
    "    # Obtener una lista de grupos únicos\n",
    "    grupos_unicos = agrupaciones['grupo'].unique()\n",
    "\n",
    "    # Dividir grupos en 10 folds\n",
    "    cv_tamaño = math.ceil(len(grupos_unicos) / 10)\n",
    "    test_grupos = grupos_unicos[i * cv_tamaño:min(len(grupos_unicos), (i + 1) * cv_tamaño)]\n",
    "    \n",
    "\n",
    "    # Filtrar conjuntos de prueba y entrenamiento basados en los grupos\n",
    "    test_indices = test_grupos\n",
    "    train_indices = agrupaciones[~agrupaciones['grupo'].isin(test_grupos)].index\n",
    "    \n",
    "    # Seleccionar los datos de entrenamiento y prueba\n",
    "    test = X.loc[test_indices].sort_index()\n",
    "    test_y = Y.loc[test_indices].sort_index()\n",
    "    ind=list(test.index)\n",
    "    \n",
    "    train = X.loc[train_indices].sort_index()\n",
    "\n",
    "    # Barajar los datos de entrenamiento para evitar sesgos\n",
    "    train = train.sample(frac=1, random_state=1)\n",
    "\n",
    "    train_y = Y.loc[train.index]\n",
    "\n",
    "    train[\"label\"]=train_y[\"label\"]\n",
    "    test[\"label\"]=test_y[\"label\"]\n",
    "    train_ind=list(train.index)\n",
    "    # Devolver los conjuntos\n",
    "    return train, test, ind,train_ind\n",
    "\n",
    "\n",
    "def predict_indexes(pred):\n",
    "\n",
    "    # Extract logits from predictions\n",
    "    start_logits, end_logits = pred.predictions\n",
    "\n",
    "    \n",
    "    # Get the best start and end indices\n",
    "    start_indexes = np.argmax(start_logits, axis=1)\n",
    "    end_indexes = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    \n",
    "    return start_indexes, end_indexes\n",
    "\n",
    "def extract_answer_from_tokens(tokenized_inputs, start_index, end_index):\n",
    "    \"\"\"\n",
    "    Extracts the predicted answer using tokenized input and index positions.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_inputs: The tokenized dataset\n",
    "        start_index: Predicted start position\n",
    "        end_index: Predicted end position\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text or \"No response\" if CLS token is chosen\n",
    "    \"\"\"\n",
    "    # Convert token IDs back to words\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "    \n",
    "    \n",
    "    # If CLS token is chosen (indicating no answer)\n",
    "    if start_index == 0 or end_index == 0 or start_index > end_index:\n",
    "        return \"no response\"\n",
    "\n",
    "    # Extract the predicted text\n",
    "    answer_tokens = tokens[start_index:end_index+1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "    \n",
    "\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def cross_validation(X, Y):\n",
    "    import torch\n",
    "    from transformers import AutoModelForQuestionAnswering, Trainer\n",
    "    from datasets import Dataset\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    predicted = []\n",
    "    tested = []\n",
    "    acc_cv = []\n",
    "    ind_cv={tipo:[] for tipo in questions}\n",
    "    preds_category_cv={tipo:[] for tipo in questions}\n",
    "    labels_category_cv={tipo:[] for tipo in questions}\n",
    "    accuracies_cv={tipo:[] for tipo in questions}\n",
    "\n",
    "    for i in range(10):\n",
    "        train, test, ind,train_ind = test_train_split(i, X, Y)\n",
    "        print(len(train), len(test))\n",
    "\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        train_data = Dataset.from_pandas(train)\n",
    "        test_data = Dataset.from_pandas(test)\n",
    "\n",
    "        # Tokenize datasets\n",
    "\n",
    "        train_data= train_data.map(tokenize_function, batched=True)\n",
    "        train_data = train_data.rename_column(\"label\", \"labels\")\n",
    "        train_data = train_data.remove_columns([\"text\"])\n",
    "        train_data.set_format(\"torch\")\n",
    "\n",
    "\n",
    "        print(\"ERRORES TEST\")\n",
    "        test_data = test_data.map(tokenize_function, batched=True)\n",
    "        test_data = test_data.rename_column(\"label\", \"labels\")\n",
    "        test_data = test_data.remove_columns([\"text\"])\n",
    "        test_data.set_format(\"torch\")\n",
    "        # for j, indice in enumerate(train_ind):\n",
    "        #     print(j,indice)\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=test_data\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # **Make Predictions**\n",
    "        pred = trainer.predict(test_data)\n",
    "        start_indexes, end_indexes = predict_indexes(pred)\n",
    "\n",
    "        answers = []\n",
    "        for j in range(len(start_indexes)):\n",
    "            # Extract answer from tokenized test data\n",
    "            tokenized_example = test_data[j]\n",
    "            answer = extract_answer_from_tokens(tokenized_example, start_indexes[j], end_indexes[j])\n",
    "            answers.append(answer)\n",
    "            # print(f\"Extracted Answer {j+1}: {answer}\")\n",
    "\n",
    "        test_data={}\n",
    "        test_label={}\n",
    "        for tipo in questions:\n",
    "            #Primero creamos la lista y luego vemos que no esté vacía para hacer el stack\n",
    "            data=[output for j,output in enumerate(answers) if re.search(rf\"_{tipo}(_\\d+)?$\", ind[j])]\n",
    "            print(data)\n",
    "            \n",
    "            if data:\n",
    "                test_data[tipo]=data\n",
    "                \n",
    "                del data\n",
    "                \n",
    "\n",
    "            labels=[output for j,output in enumerate(test[\"label\"])  if re.search(rf\"_{tipo}(_\\d+)?$\", ind[j])]\n",
    "            \n",
    "            if labels:\n",
    "                #Convertimos a token y luego str para que tenga el mismo formato que las respuestas.\n",
    "                test_label[tipo] = [\n",
    "                tokenizer.convert_tokens_to_string(tokenizer.tokenize(lab)).strip() \n",
    "                for lab in labels\n",
    "            ]\n",
    "                print(test_label[tipo])\n",
    "                \n",
    "        ind_fold={tipo: [key for key in ind if re.search(rf\"_{tipo}(_\\d+)?$\", key)] for tipo in questions}\n",
    "        \n",
    "        for tipo in questions:\n",
    "            ind_cv[tipo].append(ind_fold[tipo])\n",
    "        # **Ensure Label Comparison Works Correctly**\n",
    "        ground_truths = test[\"label\"].tolist()\n",
    "\n",
    "        for tipo in questions:\n",
    "            if tipo in test_data:\n",
    "                acc = accuracy_score(test_label[tipo], test_data[tipo])\n",
    "                accuracies_cv[tipo].append(acc)\n",
    "                print(f\"Accuracy for {tipo}: {acc:.4f}\")\n",
    "                preds_category_cv[tipo]+=test_data[tipo]\n",
    "                labels_category_cv[tipo]+=test_label[tipo]\n",
    "\n",
    "        # **Evaluate Accuracy**\n",
    "        accuracy = accuracy_score(ground_truths, answers)\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        # **Visualize Errors**\n",
    "        valid_dataset = [examples_raw[key] for key in ind]\n",
    "        visualize_errors(valid_dataset, np.array(ground_truths), answers, ind)\n",
    "\n",
    "    accuracies=[]    \n",
    "    output_dic={}\n",
    "    output_dic_t={}\n",
    "    for tipo in questions:\n",
    "\n",
    "        ind_cv[tipo]=np.concatenate(ind_cv[tipo])\n",
    "        acc = accuracy_score(labels_category_cv[tipo], preds_category_cv[tipo])\n",
    "        print(f\"Accuracy for {tipo}: {acc:.4f}\")\n",
    "        print(f\"Accuracy std for {tipo}: {np.std(accuracies_cv[tipo])}\")\n",
    "        accuracies.append(acc)\n",
    "        output_dic[tipo]={ind:preds_category_cv[tipo][i] for i, ind in enumerate(ind_cv[tipo])}\n",
    "        output_dic_t[tipo]={ind:labels_category_cv[tipo][i] for i, ind in enumerate(ind_cv[tipo])}\n",
    "        with open(f\"../Generativos/truth_dic/{tipo}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(output_dic_t[tipo], file)\n",
    "        \n",
    "    \n",
    "    return accuracies,output_dic,output_dic_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
